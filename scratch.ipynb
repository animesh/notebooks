{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/jaswindersingh2/SPOT-RNA/\n",
    "#https://tensorboard.dev/#get-started\n",
    "#!pip install -U tensorboard --user\n",
    "#/home/ash022/.local/bin/tensorboard dev upload --logdir /mnt/promec-ns9036k/.tools/guiding-cow-tensorboard/20191030-172602/\n",
    "#https://tensorboard.dev/experiment/PQnexLz9S06F0Yvykilxdw\n",
    "!pwd\n",
    "#https://www.youtube.com/watch?v=B4p6gvPs-gM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) MPCTensor(\n",
      "\t_tensor=tensor([131072, 196608, 262144])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ") tensor([3., 5., 7.])\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/facebookresearch/CrypTen.git\n",
    "#!cd CrypTen\n",
    "#!pip install -e .\n",
    "import torch\n",
    "import crypten\n",
    "crypten.init()\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "x_enc = crypten.cryptensor(x) # encrypt\n",
    "x_dec = x_enc.get_plain_text() # decrypt\n",
    "y_enc = crypten.cryptensor([2.0, 3.0, 4.0])\n",
    "sum_xy = x_enc + y_enc # add encrypted tensors\n",
    "sum_xy_dec = sum_xy.get_plain_text() # decrypt sum\n",
    "print(x_dec,y_enc,sum_xy_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/facebookresearch/CrypTen/tree/master/examples/mpc_linear_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import crypten\n",
    "import torch\n",
    "from examples.meters import AverageMeter\n",
    "\n",
    "\n",
    "def train_linear_svm(features, labels, epochs=50, lr=0.5, print_time=False):\n",
    "    # Initialize random weights\n",
    "    w = features.new(torch.randn(1, features.size(0)))\n",
    "    b = features.new(torch.randn(1))\n",
    "\n",
    "    if print_time:\n",
    "        pt_time = AverageMeter()\n",
    "        end = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward\n",
    "        label_predictions = w.matmul(features).add(b).sign()\n",
    "\n",
    "        # Compute accuracy\n",
    "        correct = label_predictions.mul(labels)\n",
    "        accuracy = correct.add(1).div(2).mean()\n",
    "        if crypten.is_encrypted_tensor(accuracy):\n",
    "            accuracy = accuracy.get_plain_text()\n",
    "\n",
    "        # Print Accuracy once\n",
    "        if crypten.communicator.get().get_rank() == 0:\n",
    "            logging.info(\n",
    "                f\"Epoch {epoch} --- Training Accuracy %.2f%%\" % (accuracy.item() * 100)\n",
    "            )\n",
    "\n",
    "        # Backward\n",
    "        loss_grad = -labels * (1 - correct) * 0.5  # Hinge loss\n",
    "        b_grad = loss_grad.mean()\n",
    "        w_grad = loss_grad.matmul(features.t()).div(loss_grad.size(1))\n",
    "\n",
    "        # Update\n",
    "        w -= w_grad * lr\n",
    "        b -= b_grad * lr\n",
    "\n",
    "        if print_time:\n",
    "            iter_time = time.time() - end\n",
    "            pt_time.add(iter_time)\n",
    "            logging.info(\"    Time %.6f (%.6f)\" % (iter_time, pt_time.value()))\n",
    "            end = time.time()\n",
    "\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def evaluate_linear_svm(features, labels, w, b):\n",
    "    \"\"\"Compute accuracy on a test set\"\"\"\n",
    "    predictions = w.matmul(features).add(b).sign()\n",
    "    correct = predictions.mul(labels)\n",
    "    accuracy = correct.add(1).div(2).mean().get_plain_text()\n",
    "    if crypten.communicator.get().get_rank() == 0:\n",
    "        print(\"Test accuracy %.2f%%\" % (accuracy.item() * 100))\n",
    "\n",
    "\n",
    "def run_mpc_linear_svm(\n",
    "    epochs=50, examples=50, features=100, lr=0.5, skip_plaintext=False\n",
    "):\n",
    "    crypten.init()\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    # Initialize x, y, w, b\n",
    "    x = torch.randn(features, examples)\n",
    "    w_true = torch.randn(1, features)\n",
    "    b_true = torch.randn(1)\n",
    "    y = w_true.matmul(x) + b_true\n",
    "    y = y.sign()\n",
    "\n",
    "    if not skip_plaintext:\n",
    "        logging.info(\"==================\")\n",
    "        logging.info(\"PyTorch Training\")\n",
    "        logging.info(\"==================\")\n",
    "        w_torch, b_torch = train_linear_svm(x, y, lr=lr, print_time=True)\n",
    "\n",
    "    # Encrypt features / labels\n",
    "    x = crypten.cryptensor(x)\n",
    "    y = crypten.cryptensor(y)\n",
    "\n",
    "    logging.info(\"==================\")\n",
    "    logging.info(\"CrypTen Training\")\n",
    "    logging.info(\"==================\")\n",
    "    w, b = train_linear_svm(x, y, lr=lr, print_time=True)\n",
    "\n",
    "    if not skip_plaintext:\n",
    "        logging.info(\"PyTorch Weights  :\")\n",
    "        logging.info(w_torch)\n",
    "    logging.info(\"CrypTen Weights:\")\n",
    "    logging.info(w.get_plain_text())\n",
    "\n",
    "    if not skip_plaintext:\n",
    "        logging.info(\"PyTorch Bias  :\")\n",
    "        logging.info(b_torch)\n",
    "    logging.info(\"CrypTen Bias:\")\n",
    "    logging.info(b.get_plain_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from examples.multiprocess_launcher import MultiProcessLauncher\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"CrypTen Linear SVM Training\")\n",
    "parser.add_argument(\n",
    "    \"--world_size\",\n",
    "    type=int,\n",
    "    default=2,\n",
    "    help=\"The number of parties to launch. Each party acts as its own process\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\", default=50, type=int, metavar=\"N\", help=\"number of total epochs to run\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--examples\", default=50, type=int, metavar=\"N\", help=\"number of examples per epoch\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--features\",\n",
    "    default=100,\n",
    "    type=int,\n",
    "    metavar=\"N\",\n",
    "    help=\"number of features per example\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr\", \"--learning-rate\", default=0.5, type=float, help=\"initial learning rate\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--skip_plaintext\",\n",
    "    default=False,\n",
    "    action=\"store_true\",\n",
    "    help=\"skip evaluation for plaintext svm\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multiprocess\",\n",
    "    default=False,\n",
    "    action=\"store_true\",\n",
    "    help=\"Run example in multiprocess mode\",\n",
    ")\n",
    "\n",
    "\n",
    "def _run_experiment(args):\n",
    "    level = logging.INFO\n",
    "    if \"RANK\" in os.environ and os.environ[\"RANK\"] != \"0\":\n",
    "        level = logging.CRITICAL\n",
    "    logging.getLogger().setLevel(level)\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s - %(process)d - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    from mpc_linear_svm import run_mpc_linear_svm\n",
    "\n",
    "    run_mpc_linear_svm(\n",
    "        args.epochs, args.examples, args.features, args.lr, args.skip_plaintext\n",
    "    )\n",
    "\n",
    "\n",
    "def main(run_experiment):\n",
    "    args = parser.parse_args()\n",
    "    if args.multiprocess:\n",
    "        launcher = MultiProcessLauncher(args.world_size, run_experiment, args)\n",
    "        launcher.start()\n",
    "        launcher.join()\n",
    "        launcher.terminate()\n",
    "    else:\n",
    "        run_experiment(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(_run_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#git clone https://github.com/broadinstitute/CellBender.git\n",
    "#pip install -e CellBender --user\n",
    "cd ../CellBender/examples/remove_background\n",
    "python generate_tiny_10x_pbmc.py\n",
    "$HOME/.local/bin/cellbender remove-background      --input ./tiny_raw_gene_bc_matrices/GRCh38      --output ./tiny_10x_pbmc.h5      --expected-cells 500   --total-droplets-included 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from scipy.fftpack import fft\n",
    "import numpy as np\n",
    "n_fft = 1024\n",
    "hop_length = 256\n",
    "n_mels = 40\n",
    "f_min = 20\n",
    "f_max = 8000\n",
    "fmin = 20\n",
    "fmax = 8000\n",
    "n_fft = 1024\n",
    "sample_rate = 16000\n",
    "sample_rate = 16000\n",
    "clip, sample_rate = librosa.load('/home/ash022/data/NORSTORE_OSL_DISK/NS9036K/promec/promec/Animesh/HareKrishna.mp3', sr=None)\n",
    "clip = clip[:132300] # first three seconds of file\n",
    "n_fft = 1024  # frame length \n",
    "start = 45000 # start at a part of the sound thats not silence\n",
    "x = clip[start:start+n_fft]\n",
    "X = fft(x, n_fft)\n",
    "X_magnitude, X_phase = librosa.magphase(X)\n",
    "X_magnitude_db = librosa.amplitude_to_db(X_magnitude)\n",
    "stft = librosa.stft(clip, n_fft=n_fft, hop_length=hop_length)\n",
    "stft_magnitude, stft_phase = librosa.magphase(stft)\n",
    "stft_magnitude_db = librosa.amplitude_to_db(stft_magnitude)\n",
    "mel_spec = librosa.feature.melspectrogram(clip, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, sr=sample_rate, power=1.0, fmin=fmin, fmax=fmax)\n",
    "mel_spec_db = librosa.amplitude_to_db(mel_spec, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepimpute.deepImpute import deepImpute\n",
    "import pandas as pd\n",
    "data = pd.read_csv('../deepimpute/examples/test.csv', index_col=0) # dimension = (cells x genes)\n",
    "imputed = deepImpute(data, NN_lim='auto', cell_subset=1)\n",
    "#imputed = deepImpute(data, NN_lim='auto', n_cores=16, cell_subset=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/\n",
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#print(X_magnitude_db,mel_spec_db)\n",
    "plt.hist(X_magnitude_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://forums.fast.ai/t/transfer-learning-in-fast-ai-how-does-the-magic-work/55620\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend\n",
    "base_model = keras.applications.resnet_v2.ResNet50V2(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "mx = keras.layers.GlobalMaxPooling2D()(base_model.output)\n",
    "out = tf.keras.layers.Concatenate()([avg, mx])\n",
    "out = keras.layers.BatchNormalization()(out)\n",
    "out = keras.layers.Dropout(0.25)(out)\n",
    "out = keras.layers.Dense(512, activation=\"relu\")(out)\n",
    "out = keras.layers.BatchNormalization()(out)\n",
    "out = keras.layers.Dropout(0.25)(out)\n",
    "out = keras.layers.Dense(nr_classes, activation=\"softmax\")(out)\n",
    "\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.003)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from  rulsif import RULSIF\n",
    "#from cycler import cycler\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "tf.keras.backend.clear_session()\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(\"Eager:\",tf.executing_eagerly())\n",
    "print(\"GPU:\",tf.test.is_gpu_available())#:with tf.device(\"/gpu:0\"):\n",
    "#tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir=\"..\\\\notebooks\\logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/jhoward/some-dicom-gotchas-to-be-aware-of-fastai\n",
    "#python3 -m pip install torch torchvision feather-format pyarrow --upgrade --user\n",
    "#python3 -m pip install git+https://github.com/fastai/fastai_dev  --user \n",
    "from fastai2.torch_basics      import *\n",
    "from fastai2.data.all          import *\n",
    "from fastai2.test              import *\n",
    "from fastai2.medical.imaging   import *\n",
    "np.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://app.sigopt.com/docs/overview/create\n",
    "from sigopt import Connection\n",
    "conn = Connection(client_token=\"LOPPBTDVPZEWILGCUUZWCNQPHEZTKAGYYFRHGFHLVZUCQJWG\")\n",
    "conn.set_api_url(\"https://api.sigopt.com\")\n",
    "experiment = conn.experiments().create(\n",
    "  name=\"Classifier Accuracy\",\n",
    "  parameters=[\n",
    "    dict(\n",
    "      name=\"gamma\",\n",
    "      bounds=dict(\n",
    "        min=0,\n",
    "        max=50\n",
    "        ),\n",
    "      type=\"double\"\n",
    "      )\n",
    "    ],\n",
    "  metrics=[\n",
    "    dict(\n",
    "      name=\"Accuracy\",\n",
    "      objective=\"maximize\"\n",
    "      )\n",
    "    ],\n",
    "  observation_budget=30,\n",
    "  parallel_bandwidth=1,\n",
    "  project=\"sigopt-examples\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://eigenfoo.xyz/tests-as-linear/\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "correlated = pd.DataFrame()\n",
    "correlated[\"x\"] = np.linspace(0, 1)\n",
    "correlated[\"y\"] = 1.5 * correlated.x + 2 * np.random.randn(len(correlated.x))\n",
    "scaled = correlated / correlated.std()\n",
    "r, p = scipy.stats.pearsonr(correlated[\"x\"], correlated[\"y\"])\n",
    "res1 = smf.ols(formula=\"y ~ 1 + x\", data=correlated).fit()\n",
    "res2 = smf.ols(formula=\"y ~ 1 + x\", data=scaled).fit()\n",
    "#print(r,p,\"\\n\",scaled)\n",
    "plt.plot(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "url = 'https://www.uniprot.org/uploadlists/'\n",
    "\n",
    "params = {\n",
    "'from': 'ACC+ID',\n",
    "'to': 'ENSEMBL_ID',\n",
    "'format': 'tab',\n",
    "'query': 'P40925 P40926 O43175 Q9UM73 P97793'\n",
    "}\n",
    "\n",
    "data = urllib.parse.urlencode(params)\n",
    "data = data.encode('utf-8')\n",
    "req = urllib.request.Request(url, data)\n",
    "with urllib.request.urlopen(req) as f:\n",
    "   response = f.read()\n",
    "print(response.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import best\n",
    "import best.plot\n",
    "from pymc import MCMC\n",
    "# This example reproduces Figure 3 of Kruschke, J. (2012) Bayesian estimation supersedes the t test. Journal of Experimental Psychology: General. According to the article, the data were generated from t distributions of known values.\n",
    "drug = (101,100,102,104,102,97,105,105,98,101,100,123,105,103,100,95,102,106,\n",
    "        109,102,82,102,100,102,102,101,102,102,103,103,97,97,103,101,97,104,\n",
    "        96,103,124,101,101,100,101,101,104,100,101)\n",
    "placebo = (99,101,100,101,102,100,97,101,104,101,102,102,100,105,88,101,100,\n",
    "           104,100,100,100,101,102,103,97,101,101,100,101,99,101,100,100,\n",
    "           101,100,99,101,100,102,99,100,99)\n",
    "data = {'drug':drug,'placebo':placebo}\n",
    "model = best.make_model( data )\n",
    "M = MCMC(model)\n",
    "M.sample(iter=110000, burn=10000)\n",
    "fig = best.plot.make_figure(M)\n",
    "fig.savefig('smart_drug.png',dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace\n",
    "import streamlit as st\n",
    "x = st.slider('x')\n",
    "st.write(x, 'squared is', x * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.sicara.com/interpretability-deep-learning-models-tensorflow-2-0-7d4ddaa351a3\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Layer name to inspect\n",
    "layer_name = 'block3_conv1'\n",
    "epochs = 100\n",
    "step_size = 1.\n",
    "filter_index = 0\n",
    "# Create a connection between the input and the target layer\n",
    "model = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "submodel = tf.keras.models.Model([model.inputs[0]], [model.get_layer(layer_name).output])\n",
    "# Initiate random noise\n",
    "input_img_data = np.random.random((1, 224, 224, 3))\n",
    "input_img_data = (input_img_data - 0.5) * 20 + 128.\n",
    "# Cast random noise from np.float64 to tf.float32 Variable\n",
    "input_img_data = tf.Variable(tf.cast(input_img_data, tf.float32))\n",
    "# Iterate gradient ascents\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = submodel(input_img_data)\n",
    "        loss_value = tf.reduce_mean(outputs[:, :, :, filter_index])\n",
    "    grads = tape.gradient(loss_value, input_img_data)\n",
    "    normalized_grads = grads / (tf.sqrt(tf.reduce_mean(tf.square(grads))) + 1e-5)\n",
    "    input_img_data.assign_add(normalized_grads * step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.sicara.com/interpretability-deep-learning-models-tensorflow-2-0-7d4ddaa351a3\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Layer name to inspect\n",
    "layer_name = 'block3_conv1'\n",
    "epochs = 100\n",
    "step_size = 1.\n",
    "filter_index = 0\n",
    "# Create a connection between the input and the target layer\n",
    "model = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "submodel = tf.keras.models.Model([model.inputs[0]], [model.get_layer(layer_name).output])\n",
    "# Initiate random noise\n",
    "input_img_data = np.random.random((1, 224, 224, 3))\n",
    "input_img_data = (input_img_data - 0.5) * 20 + 128.\n",
    "# Cast random noise from np.float64 to tf.float32 Variable\n",
    "input_img_data = tf.Variable(tf.cast(input_img_data, tf.float32))\n",
    "# Iterate gradient ascents\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = submodel(input_img_data)\n",
    "        loss_value = tf.reduce_mean(outputs[:, :, :, filter_index])\n",
    "    grads = tape.gradient(loss_value, input_img_data)\n",
    "    normalized_grads = grads / (tf.sqrt(tf.reduce_mean(tf.square(grads))) + 1e-5)\n",
    "    input_img_data.assign_add(normalized_grads * step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://kni.me/w/HJtBHsdkdwo-S64x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/dropoutlabs/encrypted-deep-learning-training-and-predictions-with-tf-encrypted-keras-557193284f44\n",
    "#!pip3 install setuptools --user\n",
    "!pip3 install tf-encrypted --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_encrypted as tfe\n",
    "from tf_encrypted.keras.losses import BinaryCrossentropy\n",
    "from tf_encrypted.keras.optimizers import SGD\n",
    "from common import DataOwner\n",
    "num_features = 10\n",
    "batch_size = 100\n",
    "steps_per_epoch = (training_set_size // batch_size)\n",
    "epochs = 20\n",
    "# Provide encrypted training data\n",
    "data_owner = DataOwner('data-owner', batch_size)\n",
    "x_train, y_train = data_owner.provide_private_training_data()\n",
    "# Define model\n",
    "model = tfe.keras.Sequential()\n",
    "model.add(tfe.keras.layers.Dense(1, batch_input_shape=[batch_size, num_features]))\n",
    "model.add(tfe.keras.layers.Activation('sigmoid'))\n",
    "# Specify optimizer and loss\n",
    "model.compile(optimizer=SGD(lr=0.01),\n",
    "              loss=BinaryCrossentropy())\n",
    "# Start training\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=epochs,\n",
    "          steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T11:19:34.256213Z",
     "start_time": "2019-08-29T11:19:33.928593Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipyvolume as ipv\n",
    "V = np.zeros((128,128,128)) # our 3d array\n",
    "# outer box\n",
    "V[30:-30,30:-30,30:-30] = 0.75\n",
    "V[35:-35,35:-35,35:-35] = 0.0\n",
    "# inner box\n",
    "V[50:-50,50:-50,50:-50] = 0.25\n",
    "V[55:-55,55:-55,55:-55] = 0.0\n",
    "ipv.quickvolshow(V, level=[0.25, 0.75], opacity=0.03, level_width=0.1, data_min=0, data_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T16:28:12.735806Z",
     "start_time": "2019-09-01T16:28:12.729711Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://pytorch.org/docs/stable/jit.html#mixing-tracing-and-scripting\n",
    "import torch\n",
    "def foo(x, y):\n",
    "    return 2 * x + y\n",
    "traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n",
    "traced_foo\n",
    "#@torch.jit.script\n",
    "#def bar(x):\n",
    "#    return traced_foo(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T15:42:15.286694Z",
     "start_time": "2019-08-30T15:41:29.821782Z"
    }
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "train_data = [[1,3], [0,4], [1,7], [0,3]]\n",
    "train_labels = [1,0,1,1]\n",
    "catboost_pool = Pool(train_data, train_labels)\n",
    "model = CatBoostClassifier(learning_rate=0.03)\n",
    "model.fit(train_data, train_labels, verbose=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T12:59:26.850521Z",
     "start_time": "2019-09-02T12:59:15.746219Z"
    }
   },
   "outputs": [],
   "source": [
    "!ann_solo HumanPlasma_2012-08_all.splib  ./RawRead/171010_Ip_Hela_ugi.raw.intensity0.charge0.MGF  out_filename --precursor_tolerance_mass 10 --precursor_tolerance_mode ppm --fragment_mz_tolerance 10\n",
    "!head -n 2 out_filename.mztab\n",
    "!tail -n 2 out_filename.mztab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T13:06:36.393940Z",
     "start_time": "2019-09-02T13:06:22.372815Z"
    }
   },
   "outputs": [],
   "source": [
    "import faiss, numpy\n",
    "vals=[faiss.Kmeans(10, 20).train(numpy.random.rand(1000, 10).astype('float32')) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T13:07:03.641075Z",
     "start_time": "2019-09-02T13:07:03.409796Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(vals)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T13:07:24.405479Z",
     "start_time": "2019-08-30T13:07:23.429968Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/audio-classification-using-fastai-and-on-the-fly-frequency-transforms-4dbe1b540f89\n",
    "import librosa\n",
    "from scipy.fftpack import fft\n",
    "n_fft = 1024\n",
    "hop_length = 256\n",
    "n_mels = 40\n",
    "f_min = 20\n",
    "f_max = 8000\n",
    "fmin = 20\n",
    "fmax = 8000\n",
    "n_fft = 1024\n",
    "sample_rate = 16000\n",
    "sample_rate = 16000\n",
    "clip, sample_rate = librosa.load('/home/ash022/.conda/pkgs/scipy-1.1.0-py36he2b7bc3_2/lib/python3.6/site-packages/scipy/io/tests/data/test-44100Hz-le-1ch-4bytes.wav', sr=None)\n",
    "clip = clip[:132300] # first three seconds of file\n",
    "n_fft = 1024  # frame length \n",
    "start = 45000 # start at a part of the sound thats not silence\n",
    "x = clip[start:start+n_fft]\n",
    "X = fft(x, n_fft)\n",
    "X_magnitude, X_phase = librosa.magphase(X)\n",
    "X_magnitude_db = librosa.amplitude_to_db(X_magnitude)\n",
    "stft = librosa.stft(clip, n_fft=n_fft, hop_length=hop_length)\n",
    "stft_magnitude, stft_phase = librosa.magphase(stft)\n",
    "stft_magnitude_db = librosa.amplitude_to_db(stft_magnitude)\n",
    "mel_spec = librosa.feature.melspectrogram(clip, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, sr=sample_rate, power=1.0, fmin=fmin, fmax=fmax)\n",
    "mel_spec_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
    "NSYNTH_IMAGES = '.'\n",
    "instrument_family_pattern = r'(\\w+)_\\w+_\\d+-\\d+-\\d+.png$'\n",
    "data = (ImageItemList.from_folder(NSYNTH_IMAGES)\n",
    "          .split_by_folder()\n",
    "          .label_from_re(instrument_family_pattern)\n",
    "          .databunch())\n",
    "learn = create_cnn(data, models.resnet18, metrics=accuracy)\n",
    "learn.fit_one_cycle(3)\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(10, 10), dpi=60)\n",
    "#https://pytorch.org/docs/master/torch.html#torch.stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T15:54:13.534358Z",
     "start_time": "2019-08-30T15:54:13.420271Z"
    }
   },
   "outputs": [],
   "source": [
    "import vaex\n",
    "ds = vaex.example() \n",
    "import numpy as np\n",
    "# creates an expression (nothing is computed)\n",
    "r = np.sqrt(ds.x**2 + ds.y**2 + ds.z**2)\n",
    "r  # for convenience, we print out some values\n",
    "#ds.mean(ds.r, binby=ds.x, shape=32, limits=[-10, 10]) # create statistics on a regular grid (1d)\n",
    "xcounts = ds.count(binby=ds.x, limits=[-10, 10], shape=64)\n",
    "print(xcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T13:10:47.470341Z",
     "start_time": "2019-09-02T13:10:46.079806Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://github.com/llSourcell/Gaussian_Mixture_Models/blob/master/intro_to_gmm_%26_em.ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "x = np.linspace(start=-10, stop=10, num=1000)\n",
    "y = stats.norm.pdf(x, loc=0, scale=1.5)+stats.norm.pdf(x, loc=1.5, scale=.5) \n",
    "#%matplotlib inlinex = np.linspace(start=-10, stop=10, num=1000)\n",
    "#plt.plot(x, y)\n",
    "sns.distplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T13:39:20.562195Z",
     "start_time": "2019-08-30T13:38:14.485656Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://threader.app/thread/1105139360226140160\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "tf.keras.backend.clear_session() \n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(\"Eager:\",tf.executing_eagerly())\n",
    "print(\"GPU:\",tf.test.is_gpu_available())#:with tf.device(\"/gpu:0\"):\n",
    "#tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir=\"/mnt/promec-ns9036k/.tools/exciting-wildebeest-tensorboard/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import summary\n",
    "from tensorboard.plugins.custom_scalar import layout_pb2\n",
    "layout_summary = summary_lib.custom_scalar_pb(layout_pb2.Layout(\n",
    "  category=[\n",
    "    layout_pb2.Category(\n",
    "      title='losses',\n",
    "      chart=[\n",
    "          layout_pb2.Chart(\n",
    "              title='losses',\n",
    "              multiline=layout_pb2.MultilineChartContent(\n",
    "                tag=[r'loss.*'],\n",
    "              )),\n",
    "          layout_pb2.Chart(\n",
    "              title='baz',\n",
    "              margin=layout_pb2.MarginChartContent(\n",
    "                series=[\n",
    "                  layout_pb2.MarginChartContent.Series(\n",
    "                    value='loss/baz/scalar_summary',\n",
    "                    lower='baz_lower/baz/scalar_summary',\n",
    "                    upper='baz_upper/baz/scalar_summary'),\n",
    "                ],\n",
    "              )),\n",
    "      ]),\n",
    "    layout_pb2.Category(\n",
    "      title='trig functions',\n",
    "      chart=[\n",
    "          layout_pb2.Chart(\n",
    "              title='wave trig functions',\n",
    "              multiline=layout_pb2.MultilineChartContent(\n",
    "                tag=[r'trigFunctions/cosine', r'trigFunctions/sine'],\n",
    "              )),\n",
    "          # The range of tangent is different. Let's give it its own chart.\n",
    "          layout_pb2.Chart(\n",
    "              title='tan',\n",
    "              multiline=layout_pb2.MultilineChartContent(\n",
    "                tag=[r'trigFunctions/tangent'],\n",
    "              )),\n",
    "      ],\n",
    "      # This category we care less about. Let's make it initially closed.\n",
    "      closed=True),\n",
    "  ]))\n",
    "writer.add_summary(layout_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T20:22:14.371255Z",
     "start_time": "2019-08-18T20:22:08.813318Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://www.nature.com/articles/s41596-018-0073-y A comprehensive evaluation of popular proteomics software workflows for label-free proteome quantification and imputation tensorly.decomposition.robust_pca\n",
    "!pip install bbknn --user #kBET\n",
    "#SCRAN pooling with ComBat9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T20:31:20.214973Z",
     "start_time": "2019-08-18T20:31:20.118406Z"
    }
   },
   "outputs": [],
   "source": [
    "import bbknn\n",
    "#bbknn.bbknn(X2[:,0])\n",
    "#https://nbviewer.jupyter.org/github/Teichlab/bbknn/blob/master/examples/pancreas.ipynb\n",
    "distances, connectivities = bbknn.bbknn_pca_matrix(X2, X2>10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T20:31:30.185784Z",
     "start_time": "2019-08-18T20:31:30.181052Z"
    }
   },
   "outputs": [],
   "source": [
    "#plt.plot(distances)\n",
    "print(connectivities)#.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T20:04:04.493825Z",
     "start_time": "2019-08-18T20:04:04.465371Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/audio-classification-using-fastai-and-on-the-fly-frequency-transforms-4dbe1b540f89\n",
    "clip, sample_rate = librosa.load(filename, sr=None)\n",
    "clip = clip[:132300] # first three seconds of file\n",
    "n_fft = 1024  # frame length \n",
    "start = 45000 # start at a part of the sound thats not silence\n",
    "x = clip[start:start+n_fft]\n",
    "X = fft(x, n_fft)\n",
    "X_magnitude, X_phase = librosa.magphase(X)\n",
    "X_magnitude_db = librosa.amplitude_to_db(X_magnitude)\n",
    "stft = librosa.stft(clip, n_fft=n_fft, hop_length=hop_length)\n",
    "stft_magnitude, stft_phase = librosa.magphase(stft)\n",
    "stft_magnitude_db = librosa.amplitude_to_db(stft_magnitude)\n",
    "mel_spec = librosa.feature.melspectrogram(clip, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, sr=sample_rate, power=1.0, fmin=fmin, fmax=fmax)\n",
    "mel_spec_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
    "n_fft = 1024\n",
    "hop_length = 256\n",
    "n_mels = 40\n",
    "f_min = 20\n",
    "f_max = 8000\n",
    "sample_rate = 16000\n",
    "NSYNTH_IMAGES = 'data/nsynth_acoustic_images'\n",
    "instrument_family_pattern = r'(\\w+)_\\w+_\\d+-\\d+-\\d+.png$'\n",
    "data = (ImageItemList.from_folder(NSYNTH_IMAGES)\n",
    "          .split_by_folder()\n",
    "          .label_from_re(instrument_family_pattern)\n",
    "          .databunch())\n",
    "learn = create_cnn(data, models.resnet18, metrics=accuracy)\n",
    "learn.fit_one_cycle(3)\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(10, 10), dpi=60)\n",
    "#https://pytorch.org/docs/master/torch.html#torch.stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:41:06.726213Z",
     "start_time": "2019-08-11T20:41:06.206575Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://github.com/llSourcell/Gaussian_Mixture_Models/blob/master/intro_to_gmm_%26_em.ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "x = np.linspace(start=-10, stop=10, num=1000)\n",
    "y = stats.norm.pdf(x, loc=0, scale=1.5)+stats.norm.pdf(x, loc=1.5, scale=.5) \n",
    "#%matplotlib inlinex = np.linspace(start=-10, stop=10, num=1000)\n",
    "plt.plot(x, y)\n",
    "sns.distplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:41:43.069299Z",
     "start_time": "2019-08-11T20:41:43.063996Z"
    }
   },
   "outputs": [],
   "source": [
    "class Gaussian:\n",
    "    \"Model univariate Gaussian\"\n",
    "    def __init__(self, mu, sigma):\n",
    "        #mean and standard deviation\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    #probability density function\n",
    "    def pdf(self, datum):\n",
    "        \"Probability of a data point given the current parameters\"\n",
    "        u = (datum - self.mu) / abs(self.sigma)\n",
    "        y = (1 / (sqrt(2 * pi) * abs(self.sigma))) * exp(-u * u / 2)\n",
    "        return y\n",
    "    #printing model values\n",
    "    def __repr__(self):\n",
    "        return 'Gaussian({0:4.6}, {1:4.6})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:47:19.333978Z",
     "start_time": "2019-08-11T20:47:19.329682Z"
    }
   },
   "outputs": [],
   "source": [
    "data\n",
    "best_single = Gaussian(np.mean(data), np.std(data))\n",
    "print('Best single Gaussian: μ = {:.2}, σ = {:.2}'.format(best_single.mu, best_single.sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T20:24:16.947625Z",
     "start_time": "2019-08-18T20:24:16.261746Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://github.com/cc-skuehn/Manifold_Learning/blob/master/Manifold_tSNE_UMAP_Advanced.ipynb\n",
    "print(__doc__)\n",
    "\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "%matplotlib inline\n",
    "from sklearn import manifold, datasets\n",
    "import numpy as np\n",
    "\n",
    "# Next line to silence pyflakes. This import is needed.\n",
    "Axes3D\n",
    "\n",
    "# Parameters\n",
    "n_points = 1000\n",
    "n_components = 2 # for tSNE / PCA\n",
    "init_random_state=0 # for all methods, change if you like (42 is the value of choice)\n",
    "\n",
    "# Generate a random point cloud, take z component for coloring\n",
    "X2 = np.random.rand(n_points, 3)\n",
    "color2 = X2[:,2]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle(\"Random Noise %i points\"\n",
    "             % (n_points), fontsize=24)\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X2[:, 0], X2[:, 1], X2[:, 2], c=color2, cmap=plt.cm.Spectral,s=125)\n",
    "#ax.view_init(4, -72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:47:50.299406Z",
     "start_time": "2019-08-11T20:47:49.064111Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import random_projection\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"PCA and Random Projections with %i points, %i components\"\n",
    "             % (n_points, n_components), fontsize=14)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "Y = pca.fit_transform(X2)\n",
    "\n",
    "ax = fig.add_subplot(131)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color2, cmap=plt.cm.Spectral)\n",
    "plt.title(\"%s \" % ('PCA'))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "# Gaussian Random Projection - GRP\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=2, random_state=init_random_state)\n",
    "Y = transformer.fit_transform(X2)\n",
    "    \n",
    "ax = fig.add_subplot(132)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color2, cmap=plt.cm.Spectral)\n",
    "plt.title(\"%s \" % ('GRP'))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "# Sparse Random ProjectiSRP\n",
    "transformer = random_projection.SparseRandomProjection(n_components=2, random_state=init_random_state)\n",
    "Y = transformer.fit_transform(X2)\n",
    "    \n",
    "ax = fig.add_subplot(133)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color2, cmap=plt.cm.Spectral)\n",
    "plt.title(\"%s \" % ('SRP'))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:48:17.765909Z",
     "start_time": "2019-08-11T20:48:13.296451Z"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "tsne = manifold.TSNE(n_components=2, random_state=1, perplexity=10)\n",
    "Y = tsne.fit_transform(X2)\n",
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:48:23.538158Z",
     "start_time": "2019-08-11T20:48:22.931009Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle(\"t-SNE applied to Random Noise with %i points\"\n",
    "             % (n_points), fontsize=14)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color2, cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:49:53.721611Z",
     "start_time": "2019-08-11T20:49:53.705909Z"
    }
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "t0 = time()\n",
    "Y= umap.UMAP(n_neighbors=15,\n",
    "                      min_dist=0.3,\n",
    "                      metric='euclidean').fit_transform(X2)\n",
    "t1 = time()\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle(\"UMAP applied to Random Noise with %i points\"\n",
    "             % (n_points), fontsize=14)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color2, cmap=plt.cm.Spectral)\n",
    "plt.title(\"UMAP (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:49:24.339172Z",
     "start_time": "2019-08-11T20:49:22.015202Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install biopython --user\n",
    "!pip install umap --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098\n",
    "#https://github.com/cran2367/autoencoder_classifier/blob/master/autoencoder_classifier.ipynb\n",
    "#https://github.com/Microsoft/dowhy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T18:25:33.415529Z",
     "start_time": "2019-08-11T18:25:33.274159Z"
    }
   },
   "outputs": [],
   "source": [
    "import Bio\n",
    "from Bio.Seq import Seq\n",
    "seq = Seq(\"AGTACACTGGT\")\n",
    "print(seq.complement()[::-1], seq.reverse_complement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%perl\n",
    "$revseq=reverse(\"AGTACACTGGT\");#=~tr/ATCG/TAGC/;\n",
    "#reverse($revseq)=~tr/ATCG/TAGC/;\n",
    "print $revseq,\"\\n\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "fasta = Seq(\"AGTACACTGGT\")\n",
    "SEQ_LEN = len(fasta)\n",
    "rowStrand = 'Rev'\n",
    "rowStart=len(fasta)\n",
    "seq = fasta[end-SEQ_LEN:end].__str__()\n",
    "if rowStrand == 'Rev':\n",
    "  end = rowStart\n",
    "  seq = fasta[end-SEQ_LEN:end].complement().__str__()\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/verilylifesciences/variant-annotation/blob/master/interactive/InteractiveVariantAnnotation.ipynb\n",
    "#https://www.ensembl.org/info/docs/tools/vep/vep_formats.html#input\n",
    "\n",
    "%%bq query\n",
    "#standardSQL\n",
    "  --\n",
    "  -- Return variants for sample NA12878 that are:\n",
    "  --   annotated as 'pathogenic' or 'other' in ClinVar\n",
    "  --   with observed population frequency less than 5%\n",
    "  --\n",
    "  WITH sample_variants AS (\n",
    "  SELECT\n",
    "    -- Remove the 'chr' prefix from the reference name.\n",
    "    REGEXP_EXTRACT(reference_name, r'chr(.+)') AS chr,\n",
    "    start,\n",
    "    reference_bases,\n",
    "    alt,\n",
    "    call.call_set_name\n",
    "  FROM\n",
    "    `genomics-public-data.platinum_genomes_deepvariant.single_sample_genome_calls` v,\n",
    "    v.call call,\n",
    "    v.alternate_bases alt WITH OFFSET alt_offset\n",
    "  WHERE\n",
    "    call_set_name = 'NA12878_ERR194147'\n",
    "    -- Require that at least one genotype matches this alternate.\n",
    "    AND EXISTS (SELECT gt FROM UNNEST(call.genotype) gt WHERE gt = alt_offset+1)\n",
    "    ),\n",
    "  --\n",
    "  --\n",
    "  rare_pathenogenic_variants AS (\n",
    "  SELECT\n",
    "    -- ClinVar does not use the 'chr' prefix for reference names.\n",
    "    reference_name AS chr,\n",
    "    start,\n",
    "    reference_bases,\n",
    "    alt,\n",
    "    CLNHGVS,\n",
    "    CLNALLE,\n",
    "    CLNSRC,\n",
    "    CLNORIGIN,\n",
    "    CLNSRCID,\n",
    "    CLNSIG,\n",
    "    CLNDSDB,\n",
    "    CLNDSDBID,\n",
    "    CLNDBN,\n",
    "    CLNREVSTAT,\n",
    "    CLNACC\n",
    "  FROM\n",
    "    `bigquery-public-data.human_variant_annotation.ncbi_clinvar_hg38_20170705` v,\n",
    "    v.alternate_bases alt\n",
    "  WHERE\n",
    "    -- Variant Clinical Significance, 0 - Uncertain significance, 1 - not provided,\n",
    "    -- 2 - Benign, 3 - Likely benign, 4 - Likely pathogenic, 5 - Pathogenic,\n",
    "    -- 6 - drug response, 7 - histocompatibility, 255 - other\n",
    "    EXISTS (SELECT sig FROM UNNEST(CLNSIG) sig WHERE REGEXP_CONTAINS(sig, '(4|5|255)'))\n",
    "    -- TRUE if >5% minor allele frequency in 1+ populations\n",
    "    AND G5 IS NULL\n",
    ")\n",
    " --\n",
    " --\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  sample_variants\n",
    "JOIN\n",
    "  rare_pathenogenic_variants USING(chr,\n",
    "    start,\n",
    "    reference_bases,\n",
    "    alt)\n",
    "ORDER BY\n",
    "  chr,\n",
    "  start,\n",
    "  reference_bases,\n",
    "  alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "home=Path.home()\n",
    "pathFiles = home / Path('promec/Elite/LARS/2017/april')\n",
    "fileName='evidence.txt'\n",
    "trainList=list(pathFiles.rglob(fileName))\n",
    "print(pathFiles,trainList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()\n",
    "for f in trainList:\n",
    "    peptideHits=pd.read_csv(f,low_memory=False,sep='\\t')\n",
    "    print(f)\n",
    "    peptideHits['Name']=f\n",
    "    df=pd.concat([df,peptideHits],sort=False)\n",
    "#print(df.head(1))\n",
    "#print(df.columns)\n",
    "colStrName=\"Raw file\"\n",
    "print(df.columns.get_loc(colStrName))\n",
    "dfDP=df.loc[:, df.columns.str.startswith(colStrName)]\n",
    "dfDP=dfDP[dfDP[colStrName].notnull()]\n",
    "#dfDP=dfDP.rename(columns = lambda x : str(x)[3:])\n",
    "dfDPcnt=dfDP[colStrName].value_counts()\n",
    "dfDPcnt[dfDPcnt>10].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import curve_fit as cf\n",
    "import ipywidgets\n",
    "ipywidgets.__version__\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "def f(x):\n",
    "    return x\n",
    "interact(f, x=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = 25\n",
    "x=np.linspace(-2,2,N_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,a,mu,sigma):\n",
    "    r=a*np.exp(-(x-mu)**2/(2*sigma**2))\n",
    "    return (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(amplitude,ideal_mu,ideal_sigma,noise_sd,noise_mean):\n",
    "    r=amplitude*np.exp(-(x-ideal_mu)**2/(2*ideal_sigma**2))\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x,r,c='k',lw=3)\n",
    "    r= r+np.random.normal(loc=noise_mean,scale=noise_sd,size=N_samples)\n",
    "    plt.scatter(x,r,edgecolors='k',c='yellow',s=60)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=interactive(func,amplitude=[1,2,3,4,5],ideal_mu=(-5,5,0.5),\n",
    "              ideal_sigma=(0,2,0.2),\n",
    "              noise_sd=(0,1,0.1),noise_mean=(-1,1,0.2))\n",
    "display(y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve-fit optimizer\n",
    "p = y.result\n",
    "p1,_=cf(f,xdata=x,ydata=p)\n",
    "par_lst=list(p1)\n",
    "\n",
    "# Gather ideal parameters from the interactive widget object\n",
    "p2=list(y.kwargs.values())\n",
    "p3=p2[0:3]\n",
    "\n",
    "# Make the data matrix\n",
    "data = np.array([p3,par_lst])\n",
    "\n",
    "# Put it in a Data Frame\n",
    "df=pd.DataFrame(data=data.T,index=['Amplitude','Mean','Std.Dev'],columns=['Original Params','Estimated Params'])\n",
    "print(df)\n",
    "\n",
    "# Plot the ideal and estimated curves\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x,f(x,par_lst[0],par_lst[1],par_lst[2]),'k--',lw=2)\n",
    "plt.plot(x,f(x,p3[0],p3[1],p3[2]),c='green',lw=2)\n",
    "plt.legend(['Fitted curve with noise','Ideal curve w/o noise'],fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%perl\n",
    "$c=0;\n",
    "while($c<10){$c++;@b=qw/A T G C/;print $b[int(rand(4))]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "#plot(sin(1:100))# Hide warnings if there are any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Load in the r magic\n",
    "# We need ggplot2\n",
    "%R require(ggplot2)\n",
    "# Load in the pandas library\n",
    "import pandas as pd \n",
    "# Make a pandas DataFrame\n",
    "df = pd.DataFrame({'Alphabet': ['a', 'b', 'c', 'd','e', 'f', 'g', 'h','i'],\n",
    "                   'A': [4, 3, 5, 2, 1, 7, 7, 5, 9],\n",
    "                   'B': [0, 4, 3, 6, 7, 10,11, 9, 13],\n",
    "                   'C': [1, 2, 3, 1, 2, 3, 1, 2, 3]})\n",
    "# Take the name of input variable df and assign it to an R variable of the same name\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "require(ggplot2)\n",
    "plot(sin(1:100))\n",
    "ggplot(x=sin(1:100),y=(1:100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%perl\n",
    "use Statistics::R;\n",
    "  my $R = Statistics::R->new();\n",
    "  my $output_file = \"file.ps\";\n",
    "  $R->run(qq`postscript(\"$output_file\", horizontal=FALSE, width=500, height=500)`);\n",
    "  $R->run(q`plot(c(1, 5, 10), type = \"l\")`);\n",
    "  $R->run(q`dev.off()`);\n",
    "  my $input_value = 5;\n",
    "  $R->set('x', $input_value);\n",
    "  $R->run(q`y <- x^2`);\n",
    "  my $output_value = $R->get('y');\n",
    "  print \"y = $output_value\\n\";\n",
    "  $R->stop();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/bloomberg/bqplot/blob/master/examples/Applications/Outlier%20Detection.ipynb\n",
    "from scipy.stats import percentileofscore\n",
    "from scipy.interpolate import interp1d\n",
    "import bqplot.pyplot as plt\n",
    "from bqplot import *\n",
    "from traitlets import List, Float, observe\n",
    "from ipywidgets import IntRangeSlider, Layout, VBox, HBox, jslink\n",
    "from pandas import DatetimeIndex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def quantile_space(x, q1=0.1, q2=0.9):\n",
    "    '''\n",
    "    Returns a function that squashes quantiles between q1 and q2\n",
    "    '''\n",
    "    q1_x, q2_x = np.percentile(x, [q1, q2])\n",
    "    qs = np.percentile(x, np.linspace(0, 100, 100))\n",
    "    def get_quantile(t):\n",
    "        return np.interp(t, qs, np.linspace(0, 100, 100))\n",
    "    def f(y):\n",
    "        return np.interp(get_quantile(y), [0, q1, q2, 100], [-1, 0, 0, 1])\n",
    "    return f\n",
    "\n",
    "class DNA(VBox):\n",
    "    \n",
    "    colors = List()\n",
    "    q1 = Float()\n",
    "    q2 = Float()\n",
    "    \n",
    "    def __init__(self, data, **kwargs):\n",
    "        self.data = data\n",
    "        date_x, date_y = False, False\n",
    "        transpose = kwargs.pop('transpose', False)\n",
    "        if transpose is True:\n",
    "            if type(data.index) is DatetimeIndex:\n",
    "                self.x_scale = DateScale()\n",
    "            if type(data.columns) is DatetimeIndex:\n",
    "                self.y_scale = DateScale()\n",
    "            x, y = list(data.columns.values), data.index.values\n",
    "        else:\n",
    "            if type(data.index) is DatetimeIndex:\n",
    "                date_x = True\n",
    "            if type(data.columns) is DatetimeIndex:\n",
    "                date_y = True\n",
    "            x, y = data.index.values, list(data.columns.values)\n",
    "            \n",
    "        self.q1, self.q2 = kwargs.pop('quantiles', (1, 99))\n",
    "        \n",
    "        self.quant_func = quantile_space(self.data.values.flatten(), q1=self.q1, q2=self.q2)\n",
    "        self.colors = kwargs.pop('colors', ['Red', 'Black', 'Green'])\n",
    "        \n",
    "        self.x_scale = DateScale() if date_x is True else LinearScale()\n",
    "        self.y_scale = DateScale() if date_y is True else OrdinalScale(padding_y=0)\n",
    "        self.color_scale = ColorScale(colors=self.colors)\n",
    "        self.heat_map = HeatMap(color=self.quant_func(self.data.T), x=x, y=y, scales={'x': self.x_scale, 'y': self.y_scale,\n",
    "                                                                               'color': self.color_scale})\n",
    "        self.x_ax = Axis(scale=self.x_scale)\n",
    "        self.y_ax = Axis(scale=self.y_scale, orientation='vertical')\n",
    "        show_axes = kwargs.pop('show_axes', True)\n",
    "        self.axes = [self.x_ax, self.y_ax] if show_axes is True else []\n",
    "        \n",
    "        self.height = kwargs.pop('height', '800px')\n",
    "        self.layout = kwargs.pop('layout', Layout(width='100%', height=self.height, flex='1'))\n",
    "        self.fig_margin = kwargs.pop('fig_margin', {'top': 60, 'bottom': 60, 'left': 150, 'right': 0})\n",
    "        kwargs.setdefault('padding_y', 0.0)\n",
    "        \n",
    "        self.create_interaction(**kwargs)\n",
    "        \n",
    "        self.figure = Figure(marks=[self.heat_map], axes=self.axes, fig_margin=self.fig_margin, \n",
    "                             layout=self.layout, min_aspect_ratio=0.,**kwargs)\n",
    "        \n",
    "        super(VBox, self).__init__(children=[self.range_slider, self.figure], layout=Layout(align_items='center',\n",
    "                                                                                           width='100%',\n",
    "                                                                                           height='100%'),\n",
    "                                   **kwargs)\n",
    "        \n",
    "    def create_interaction(self, **kwargs):\n",
    "        self.range_slider = IntRangeSlider(description='Filter Range', value=(self.q1, self.q2), layout=Layout(width='100%'))\n",
    "        self.range_slider.observe(self.slid_changed, 'value')\n",
    "        self.observe(self.changed, ['q1', 'q2'])\n",
    "        \n",
    "        \n",
    "    def slid_changed(self, new):\n",
    "        self.q1 = self.range_slider.value[0]\n",
    "        self.q2 = self.range_slider.value[1]\n",
    "        \n",
    "    def changed(self, new):\n",
    "        self.range_slider.value = (self.q1, self.q2)\n",
    "        \n",
    "        self.quant_func = quantile_space(self.data.values.flatten(), q1=self.q1, q2=self.q2)\n",
    "        self.heat_map.color = self.quant_func(self.data.T)\n",
    "        \n",
    "    def get_filtered_df(self, fill_type='median'):\n",
    "        q1_x, q2_x = np.percentile(self.data, [self.q1, self.q2])\n",
    "        if fill_type == 'median':\n",
    "            return self.data[(self.data >= q1_x) & (self.data <= q2_x)].apply(lambda x: x.fillna(x.median()))\n",
    "        elif fill_type == 'mean':\n",
    "            return self.data[(self.data >= q1_x) & (self.data <= q2_x)].apply(lambda x: x.fillna(x.mean()))\n",
    "        else:\n",
    "            raise ValueError(\"fill_type must be one of ('median', 'mean')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100\n",
    "def num_to_col_letters(num):\n",
    "    letters = ''\n",
    "    while num:\n",
    "        mod = (num - 1) % 26\n",
    "        letters += chr(mod + 65)\n",
    "        num = (num - 1) // 26\n",
    "    return ''.join(reversed(letters))\n",
    "\n",
    "letters = []\n",
    "\n",
    "for i in range(1, size+1):\n",
    "    letters.append(num_to_col_letters(i))\n",
    "\n",
    "data = pd.DataFrame(np.random.randn(size, size), columns=letters)\n",
    "\n",
    "data_dna = DNA(data, title='DNA of our Data', height='1400px', colors=['Red', 'White', 'Green'])\n",
    "data_dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dna.q1, data_dna.q2 = 5, 95\n",
    "data_clean = data_dna.get_filtered_df()\n",
    "data_mean = data_dna.get_filtered_df(fill_type='mean')\n",
    "DNA(data_clean, title='Cleaned Data', height='1200px', colors=['Red', 'White', 'Green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html?utm_campaign=devshow_series_eagerexecutionfortensorflow_111017&utm_source=gdev&utm_medium=yt-desc\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(25)\n",
    "counter = 0\n",
    "while not tf.equal(a, 1):\n",
    "  if tf.equal(a % 2, 0):\n",
    "    a = a / 2\n",
    "  else:\n",
    "    a = 3 * a + 1\n",
    "  print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "  return tf.multiply(x, x)\n",
    "\n",
    "grad = tfe.gradients_function(square)\n",
    "\n",
    "print(square(3.))    # [9.]\n",
    "print(grad(3.))      # [6.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradgrad = tfe.gradients_function(lambda x: grad(x)[0])\n",
    "\n",
    "print(gradgrad(3.))  # [2.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs(x):\n",
    "  return x if x > 0. else -x\n",
    "\n",
    "grad = tfe.gradients_function(abs)\n",
    "\n",
    "print(grad(2.0))  # [1.]\n",
    "print(grad(-2.0)) # [-1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log1pexp(x):\n",
    "  return tf.log(1 + tf.exp(x))\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# The gradient computation works fine at x = 0.\n",
    "print(grad_log1pexp(0.))\n",
    "# [0.5]\n",
    "# However it returns a `nan` at x = 100 due to numerical instability.\n",
    "print(grad_log1pexp(100.))\n",
    "# [nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tfe.custom_gradient\n",
    "def log1pexp(x):\n",
    "  e = tf.exp(x)\n",
    "  def grad(dy):\n",
    "    return dy * (1 - 1 / (1 + e))\n",
    "  return tf.log(1 + e), grad\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# Gradient at x = 0 works as before.\n",
    "print(grad_log1pexp(0.))\n",
    "# [0.5]\n",
    "# And now gradient computation at x=100 works as well.\n",
    "print(grad_log1pexp(100.))\n",
    "# [1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tfe.Network):\n",
    "  def __init__(self):\n",
    "    super(MNISTModel, self).__init__()\n",
    "    self.layer1 = self.track_layer(tf.layers.Dense(units=10))\n",
    "    self.layer2 = self.track_layer(tf.layers.Dense(units=10))\n",
    "  def call(self, input):\n",
    "    \"\"\"Actually runs the model.\"\"\"\n",
    "    result = self.layer1(input)\n",
    "    result = self.layer2(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make up a blank input image\n",
    "model = MNISTModel()\n",
    "batch = tf.zeros([1, 1, 784])\n",
    "print(batch.shape)\n",
    "# (1, 1, 784)\n",
    "result = model(batch)\n",
    "print(result)\n",
    "# tf.Tensor([[[ 0.  0., ...., 0.]]], shape=(1, 1, 10), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, x, y):\n",
    "  y_ = model(x)\n",
    "  return tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_) \n",
    "\n",
    "#And then, our training loop:\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "for (x, y) in tfe.Iterator(dataset):\n",
    "  grads = tfe.implicit_gradients(loss_function)(model, x, y)\n",
    "  optimizer.apply_gradients(grads)\n",
    "\n",
    "    #implicit_gradients() \n",
    "\n",
    "    #calculates the derivatives of loss_function with respect to all the TensorFlow variables used during its computation.\n",
    "#We can move computation to a GPU the same way we’ve always done with TensorFlow:\n",
    "with tf.device(\"/gpu:0\"):\n",
    "  for (x, y) in tfe.Iterator(dataset):\n",
    "    optimizer.minimize(lambda: loss_function(model, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
