{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T21:42:51.390669Z",
     "start_time": "2019-03-03T21:42:48.948824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/3e/599dfe7b8c35ef9c72df4825d876c023fafe5e2618483ee3f3f2f4cdc3a9/pytorch-lightning-0.0.2.tar.gz\n",
      "Collecting test-tube (from pytorch-lightning)\n",
      "  Downloading https://files.pythonhosted.org/packages/3a/50/47ea5613be804c8e6e0b01b1719e1f8186b8bc626441002b141c8a962abb/test_tube-0.631.tar.gz\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning) (1.0.1.post2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning) (4.28.1)\n",
      "Collecting pandas>=0.20.3 (from test-tube->pytorch-lightning)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/74/e50234bc82c553fecdbd566d8650801e3fe2d6d8c8d940638e3d8a7c5522/pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.1MB 651kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from test-tube->pytorch-lightning) (1.16.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from test-tube->pytorch-lightning) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.3->test-tube->pytorch-lightning) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.3->test-tube->pytorch-lightning) (2018.5)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from imageio>=2.3.0->test-tube->pytorch-lightning) (5.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas>=0.20.3->test-tube->pytorch-lightning) (1.11.0)\n",
      "Building wheels for collected packages: pytorch-lightning, test-tube\n",
      "  Building wheel for pytorch-lightning (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/notebook/.cache/pip/wheels/63/8f/d7/24693e99c7103d87e84c6b7905ff053541ffed249979cb22e5\n",
      "  Building wheel for test-tube (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/notebook/.cache/pip/wheels/69/3d/82/2374e255772a5dbb2c5b90706230802beaa66ec61664756c62\n",
      "Successfully built pytorch-lightning test-tube\n",
      "\u001b[31mplotnine 0.3.0 has requirement scipy>=1.0.0, but you'll have scipy 0.19.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmodin 0.3.1 has requirement numpy<=1.15.0, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mmodin 0.3.1 has requirement pandas==0.23.4, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mhypertools 0.5.1 has requirement scipy>=1.0.0, but you'll have scipy 0.19.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mhypertools 0.5.1 has requirement seaborn>=0.8.1, but you'll have seaborn 0.8 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pandas, test-tube, pytorch-lightning\n",
      "  Found existing installation: pandas 0.19.2\n",
      "    Uninstalling pandas-0.19.2:\n",
      "      Successfully uninstalled pandas-0.19.2\n",
      "Successfully installed pandas-0.24.2 pytorch-lightning-0.0.2 test-tube-0.631\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch torchvision #--upgrade\n",
    "!pip install  pytorch-lightning   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning.example_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cb3b87e6bd13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_parse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_default_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt_callbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExampleModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning.example_model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from test_tube import HyperOptArgumentParser, Experiment\n",
    "from pytorch_lightning.models.trainer import Trainer\n",
    "from pytorch_lightning.utils.arg_parse import add_default_args\n",
    "from pytorch_lightning.utils.pt_callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.example_model import ExampleModel\n",
    "\n",
    "\n",
    "def main(hparams):\n",
    "    \"\"\"\n",
    "    Main training routine specific for this project\n",
    "    :param hparams:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # init experiment\n",
    "    exp = Experiment(\n",
    "        name=hparams.tt_name,\n",
    "        debug=hparams.debug,\n",
    "        save_dir=hparams.tt_save_path,\n",
    "        version=hparams.hpc_exp_number,\n",
    "        autosave=False,\n",
    "        description=hparams.tt_description\n",
    "    )\n",
    "\n",
    "    exp.argparse(hparams)\n",
    "    exp.save()\n",
    "\n",
    "    model_save_path = '{}/{}/{}'.format(hparams.model_save_path, exp.name, exp.version)\n",
    "\n",
    "    # build model\n",
    "    model = ExampleModel(hparams)\n",
    "\n",
    "    # callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_acc', patience=3, mode='min', verbose=True)\n",
    "    checkpoint = ModelCheckpoint(filepath=model_save_path, save_function=None, save_best_only=True, verbose=True, monitor='val_acc', mode='min')\n",
    "\n",
    "    # configure trainer\n",
    "    trainer = Trainer(experiment=exp, checkpoint_callback=checkpoint, early_stop_callback=early_stop)\n",
    "\n",
    "    # train model\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # use default args given by lightning\n",
    "    root_dir = os.path.split(os.path.dirname(sys.modules['__main__'].__file__))[0]\n",
    "    parent_parser = HyperOptArgumentParser(strategy='random_search', add_help=False)\n",
    "    add_default_args(parent_parser, root_dir)\n",
    "\n",
    "    # allow model to overwrite or extend args\n",
    "    parser = ExampleModel.add_model_specific_args(parent_parser)\n",
    "    hyperparams = parser.parse_args()\n",
    "\n",
    "    # train model\n",
    "    main(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available to Torch: True\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'F:/OneDrive - NTNU/UTR/data2.csv' does not exist: b'F:/OneDrive - NTNU/UTR/data2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-408b133d6c55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F:/OneDrive - NTNU/UTR/data2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'F:/OneDrive - NTNU/UTR/data2.csv' does not exist: b'F:/OneDrive - NTNU/UTR/data2.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('GPU available to Torch: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('F:/OneDrive - NTNU/UTR/data2.csv')\n",
    "df.head()\n",
    "df.describe()\n",
    "\n",
    "import numpy as np\n",
    "df['RFPlog']=np.log2(data['Fluorescence'])\n",
    "df['RFPlog'].hist()\n",
    "\n",
    "df['ReadsLog']=np.log2(df['#Reads Col'])\n",
    "df['ReadsLog'].hist()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(df['RFPlog'],df['ReadsLog'])\n",
    "\n",
    "sorted_inds = df.sort_values('RFPlog').index.values\n",
    "train_inds = sorted_inds[:int(0.1*len(sorted_inds))] # 95% of the data as the training set\n",
    "test_inds = sorted_inds[int(0.1*len(sorted_inds)):] # UTRs with most reads at time point 0 as the test set\n",
    "val_idx = int(0.9*len(train_inds))\n",
    "val_inds = train_inds[val_idx:]\n",
    "train_inds = train_inds[:val_idx]\n",
    "print(len(train_inds))\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from ipywidgets import IntProgress\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, df, seq_len):\n",
    "        self.data = df\n",
    "        self.bases = ['A','C','G','T']\n",
    "        self.base_dict = dict(zip(self.bases,range(4))) # {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3}\n",
    "        self.total_width = seq_len + 20\n",
    "    def __len__(self):\n",
    "        return (self.data.shape[0])\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data.iloc[idx].UTR\n",
    "        X = np.zeros((1, 4, self.total_width))\n",
    "        y = self.data.iloc[idx].growth_rate\n",
    "        for b in range(len(seq)):\n",
    "            # this will assign a 1 to the appropriate base and position for this UTR sequence\n",
    "            X[0, self.base_dict[seq[b]], int(b + round((self.total_width - len(seq))/2.))] = 1.\n",
    "        return(seq, X, y)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(4, 13))\n",
    "        self.dropout = nn.Dropout(p=0.15)\n",
    "        self.conv2 = nn.Conv2d(128, 128, (1,13))\n",
    "        self.fc1 = nn.Linear(128 * 1 * 34, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 128 * 1 * 34)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "#net = net.cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "train_data = DNADataset(df.iloc[train_inds], seq_len=50)\n",
    "val_data = DNADataset(df.iloc[val_inds], seq_len=50)\n",
    "test_data = DNADataset(df.iloc[test_inds], seq_len=50)\n",
    "train_data_loader = DataLoader(train_data, batch_size=32,shuffle=True, num_workers=4)\n",
    "\n",
    "val_data_loader = DataLoader(val_data, batch_size=32) # Validate everything in one batch?!\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=len(test_data)) # Validate everything in one batch?!\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i_batch, sampled_batch in enumerate(tqdm(train_data_loader)):\n",
    "        sequence, transformed_sequence, growth_rate = sampled_batch\n",
    "        #inputs, labels = Variable(transformed_sequence.float().cuda()), Variable(growth_rate.float().cuda())\n",
    "        inputs, labels = Variable(transformed_sequence.float()), Variable(growth_rate.float())\n",
    "        optimizer.zero_grad()\n",
    "        net.train()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    error = 0\n",
    "    totalE = 0\n",
    "    net.eval()\n",
    "    for batch in tqdm(val_data_loader):\n",
    "      v_seq, X_v, y_v = batch\n",
    "      v_pred = net(Variable(X_v.float()))#.cuda()))\n",
    "      totalE = totalE + y_v.size(0)\n",
    "      raw_error = v_pred[:,0].data - y_v.float()#.cuda()\n",
    "      error += (raw_error**2).sum()\n",
    "      avg_mse = error / float(totalE)\n",
    "\n",
    "torch.save(net, 'saved_model.t7')\n",
    "\n",
    "error = 0\n",
    "total = 0\n",
    "for batch in tqdm(val_data_loader):\n",
    "                v_seq, X_v, y_v = batch\n",
    "                v_pred = net(Variable(X_v.float()))#.cuda()))\n",
    "                total += y_v.size(0)\n",
    "                raw_error = v_pred[:,0].data - y_v.float()#.cuda()\n",
    "                error += (raw_error**2).sum()\n",
    "\n",
    "avg_mse = error / float(total)\n",
    "print(\"Validation error: {}\".format(avg_mse))\n",
    "\n",
    "error = 0\n",
    "total = 0\n",
    "for batch in tqdm(test_data_loader):\n",
    "                v_seq, X_v, y_v = batch\n",
    "                v_pred = net(Variable(X_v.float()))#.cuda()))\n",
    "                total += y_v.size(0)\n",
    "                raw_error = v_pred[:,0].data - y_v.float()#.cuda()\n",
    "                error += (raw_error**2).sum()\n",
    "\n",
    "avg_mse = error / float(total)\n",
    "print(\"Test error: {}\".format(avg_mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T21:45:41.710587Z",
     "start_time": "2019-03-03T21:44:27.137643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai\n",
      "  Using cached https://files.pythonhosted.org/packages/98/6c/6b6d34dc783f1535190bfafbc5c0063656f06a3dd5c37b2b2b7bd0e8011e/fastai-1.0.46-py3-none-any.whl\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from fastai) (18.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from fastai) (3.0.2)\n",
      "Requirement already satisfied: numexpr in /opt/conda/lib/python3.6/site-packages (from fastai) (2.6.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from fastai) (3.13)\n",
      "Collecting torch>=1.0.0 (from fastai)\n",
      "  Using cached https://files.pythonhosted.org/packages/31/ca/dd2c64f8ab5e7985c4af6e62da933849293906edcdb70dac679c93477733/torch-1.0.1.post2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from fastai) (0.2.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from fastai) (0.23.4)\n",
      "Collecting nvidia-ml-py3 (from fastai)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from fastai) (0.6)\n",
      "Collecting fastprogress>=0.1.19 (from fastai)\n",
      "  Using cached https://files.pythonhosted.org/packages/86/30/01f597392e4e7b4982f387028da941e1fd60a8d53511d17225858d87fb22/fastprogress-0.1.20-py3-none-any.whl\n",
      "Collecting spacy>=2.0.18 (from fastai)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/6e/a89da6b5c83f8811e46e3a9270c1aed90e9b9ee6c60faf52b7239e5d3d69/spacy-2.0.18-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from fastai) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/ash022/.local/lib/python3.6/site-packages (from fastai) (1.15.0)\n",
      "Collecting bottleneck (from fastai)\n",
      "Requirement already satisfied: typing in /opt/conda/lib/python3.6/site-packages (from fastai) (3.6.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from fastai) (2.20.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.6/site-packages (from fastai) (4.6.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from fastai) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->fastai) (1.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->fastai) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->fastai) (2.7.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->fastai) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->fastai) (1.0.1)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas->fastai) (2018.7)\n",
      "Collecting preshed<2.1.0,>=2.0.1 (from spacy>=2.0.18->fastai)\n",
      "  Using cached https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.18->fastai) (0.28.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy>=2.0.18->fastai)\n",
      "  Using cached https://files.pythonhosted.org/packages/3d/61/9b0520c28eb199a4b1ca667d96dd625bba003c14c75230195f9691975f85/cymem-2.0.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting regex==2018.01.10 (from spacy>=2.0.18->fastai)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.18->fastai) (0.9.6)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.18->fastai) (0.2.8.2)\n",
      "Collecting thinc<6.13.0,>=6.12.1 (from spacy>=2.0.18->fastai)\n",
      "  Using cached https://files.pythonhosted.org/packages/db/a7/46640a46fd707aeb204aa4257a70974b6a22a0204ba703164d803215776f/thinc-6.12.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.18->fastai) (1.35)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->fastai) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->fastai) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->fastai) (2018.10.15)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->fastai) (2.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->fastai) (40.6.2)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.5.6)\n",
      "Collecting msgpack-numpy<0.4.4 (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai)\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/45/464be6da85b5ca893cfcbd5de3b31a6710f636ccb8521b17bd4110a08d94/msgpack_numpy-0.4.3.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.9.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (4.28.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (1.10.11)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.9.0)\n",
      "Installing collected packages: torch, nvidia-ml-py3, fastprogress, cymem, preshed, regex, msgpack-numpy, thinc, spacy, bottleneck, fastai\n",
      "\u001b[33m  The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/home/ash022/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed bottleneck-1.2.1 cymem-2.0.2 fastai-1.0.46 fastprogress-0.1.20 msgpack-numpy-0.4.3.2 nvidia-ml-py3-7.352.0 preshed-2.0.1 regex-2018.1.10 spacy-2.0.18 thinc-6.12.1 torch-1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "!pip install fastai --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T21:47:07.703426Z",
     "start_time": "2019-03-03T21:47:06.297023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T21:55:54.864778Z",
     "start_time": "2019-03-03T21:55:54.859297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4, 0.45], [0.5, 0.55]] [[0.15, 0.2], [0.25, 0.3]]\n"
     ]
    }
   ],
   "source": [
    "#coding https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ with pytorch, checking with iterative version at  https://github.com/animesh/ann/blob/master/ann/Program.cs with following output\n",
    "#Iteration = 1   Error = 0.298371108760003       Outputs = 0.751365069552316     0.772928465321463\n",
    "#Iteration = 2   Error = 0.291027773693599       Outputs = 0.742088111190782     0.775284968294459  ...\n",
    "\n",
    "inp=[0.05,0.10]\n",
    "inpw=[[0.15,0.20],[0.25,0.3]]\n",
    "hidw=[[0.4,0.45],[0.5,0.55]]\n",
    "outputr=[0.01,0.99]\n",
    "bias=[0.35,0.6]\n",
    "lr=0.5\n",
    "print(hidw,inpw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T21:55:57.609950Z",
     "start_time": "2019-03-03T21:55:57.603664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.84772999  0.31633614]\n",
      " [-0.85602802 -1.39846585]] [[ 0.20957657  0.20203366]\n",
      " [ 0.27245755 -0.70527726]]\n"
     ]
    }
   ],
   "source": [
    "#\"ALL YOU NEED IS A GOOD INIT\" https://arxiv.org/pdf/1511.06422.pdf\n",
    "import numpy as np\n",
    "inpw=np.random.randn(len(outputr),int(len(inp)*np.sqrt(2/len(inp))))\n",
    "hidw=np.random.randn(len(outputr),int(len(inp)*np.sqrt(2/len(inp))))\n",
    "print(inpw,hidw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T21:56:36.552219Z",
     "start_time": "2019-03-03T21:56:30.309821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "torch.Size([2]) tensor([0.0100, 0.9900], device='cuda:0', dtype=torch.float64) tensor([0.3500, 0.6000], device='cuda:0', dtype=torch.float64) torch.Size([2, 2]) tensor([[ 0.2096,  0.2020],\n",
      "        [ 0.2725, -0.7053]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "Device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(Device)\n",
    "x=torch.tensor(inp, dtype=torch.double, device=Device)\n",
    "y=torch.tensor(outputr, dtype=torch.double, device=Device)\n",
    "b=torch.tensor(bias, dtype=torch.double, device=Device)\n",
    "w1=torch.tensor(inpw, dtype=torch.double, device=Device)\n",
    "w2=torch.tensor(hidw, dtype=torch.double, device=Device)\n",
    "print(x.size(),y,b,w1.size(),w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T21:56:43.554772Z",
     "start_time": "2019-03-03T21:56:43.403376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 MSE:  tensor(0.3147, device='cuda:0', dtype=torch.float64)\n",
      "tensor([[ 1.8477,  0.3163],\n",
      "        [-0.8560, -1.3985]], device='cuda:0', dtype=torch.float64)\n",
      "tensor([[ 0.2096,  0.2020],\n",
      "        [ 0.2725, -0.7053]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "iter=0\n",
    "while iter<1:\n",
    "    iter+=1\n",
    "    h = torch.sigmoid(x.matmul(w1.transpose(0,1))+b[0])\n",
    "    y_pred = torch.sigmoid(h.matmul(w2.transpose(0,1))+b[1])\n",
    "    print(\"iteration:\",iter,\"MSE: \",0.5*(((y_pred - y).pow(2)).sum()))\n",
    "    print(w1)\n",
    "    print(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T10:53:23.577725Z",
     "start_time": "2019-02-21T10:53:23.555159Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0\n",
    "class Neural_Network(torch.nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        self.W1 = w1.transpose(0,1) # 3 x 2 tensor\n",
    "        self.W2 = w2.transpose(0,1)\n",
    "    def forward(self, x):\n",
    "        self.z = torch.matmul(x, self.W1) # 3 x 3 \".dot\" does not broadcast in PyTorch\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    def backward(self, x, y, o):\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(x, self.z2_delta.transpose(0,1)) #torch.sigmoid(x.matmul(w1.transpose(0,1))+b[0])\n",
    "        self.W2 += torch.matmul(self.z2, self.o_delta)\n",
    "    def train(self, x, y):\n",
    "        o = self.forward(x)\n",
    "        self.backward(x, y, o)\n",
    "    def saveWeights(self, model):\n",
    "        torch.save(model, \"NN\")\n",
    "    def predict(self):\n",
    "        print (\"Predicted weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
    "        print (\"Output: \\n\" + str(self.forward(xPredicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T10:53:28.753811Z",
     "start_time": "2019-02-21T10:53:28.682749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 0.24251985734837728\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-55261c524c25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# trains the NN 1,000 times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# mean sum squared loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5644b2f6dc70>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msaveWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5644b2f6dc70>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, y, o)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoidPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#torch.sigmoid(x.matmul(w1.transpose(0,1))+b[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "for i in range(1000):  # trains the NN 1,000 times\n",
    "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(x))**2).detach().item()))  # mean sum squared loss\n",
    "    NN.train(x, y)\n",
    "NN.saveWeights(NN)\n",
    "NN.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T10:03:05.969359Z",
     "start_time": "2019-02-22T10:03:03.035789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "sns.set(style='white', rc={'figure.figsize':(14, 12)})\n",
    "from sklearn.datasets import load_digits\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T10:07:52.815018Z",
     "start_time": "2019-02-22T10:07:48.202306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/cc/3b7408b8ecf7c1d20ad480c3eaed7619857bf1054b690226e906fdf14258/ffmpeg-1.4.tar.gz\n",
      "Building wheels for collected packages: ffmpeg\n",
      "  Building wheel for ffmpeg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/notebook/.cache/pip/wheels/b6/68/c3/a05a35f647ba871e5572b9bbfc0b95fd1c6637a2219f959e7a\n",
      "Successfully built ffmpeg\n",
      "Installing collected packages: ffmpeg\n",
      "Successfully installed ffmpeg-1.4\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install hypertools\n",
    "#!pip3 uninstall UMAP\n",
    "!pip3 install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T10:03:34.309372Z",
     "start_time": "2019-02-22T10:03:34.171984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "data = digits.data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T10:03:46.026298Z",
     "start_time": "2019-02-22T10:03:46.017918Z"
    }
   },
   "outputs": [],
   "source": [
    "def tween(e1, e2, n_frames=20):\n",
    "    for i in range(5):\n",
    "        yield e1\n",
    "    for i in range(n_frames):\n",
    "        alpha = i / float(n_frames - 1)\n",
    "        yield (1 - alpha) * e1 + alpha * e2\n",
    "    for i in range(5):\n",
    "        yield(e2)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T10:04:01.852639Z",
     "start_time": "2019-02-22T10:04:01.839854Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_frame_data(data, arg_name='n_neighbors', arg_list=[3,4,5,7,9,11,15,20,25,30,40,60,100]):\n",
    "    result = []\n",
    "    es = []\n",
    "    for arg in arg_list:\n",
    "        kwargs = {arg_name:arg}\n",
    "        if len(es) > 0:\n",
    "            es.append(UMAP(init=es[-1], negative_sample_rate=3, **kwargs).fit_transform(data))\n",
    "        else:\n",
    "            es.append(UMAP(negative_sample_rate=3, **kwargs).fit_transform(data))\n",
    "        \n",
    "    for e1, e2 in zip(es[:-1], es[1:]):\n",
    "        result.extend(list(tween(e1, e2)))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T10:04:15.987838Z",
     "start_time": "2019-02-22T10:04:15.963035Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_animation(frame_data, arg_name='n_neighbors', arg_list=[3,4,5,7,9,11,15,20,25,30,40,60,100]):\n",
    "    fig = plt.figure()\n",
    "    all_data = np.vstack(frame_data)\n",
    "    frame_bounds = (all_data[:, 0].min() * 1.1, \n",
    "                    all_data[:, 0].max() * 1.1,\n",
    "                    all_data[:, 1].min() * 1.1, \n",
    "                    all_data[:, 1].max() * 1.1)\n",
    "    ax = plt.axes(xlim=(frame_bounds[0], frame_bounds[1]), \n",
    "                  ylim=(frame_bounds[2], frame_bounds[3]))\n",
    "    points = plt.scatter(frame_data[0][:, 0], frame_data[0][:, 1], \n",
    "                        s=5, c=digits.target, cmap='Spectral', animated=True)\n",
    "    title = plt.title('', fontsize=24)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    cbar = plt.colorbar(values=np.arange(10), boundaries=np.arange(11)-0.5, ticks=np.arange(10), drawedges=True)\n",
    "    cbar.ax.yaxis.set_ticklabels(np.arange(10), fontsize=18)\n",
    "\n",
    "    def init():\n",
    "        points.set_offsets(frame_data[0])\n",
    "        title.set_text('UMAP with {}={}'.format(arg_name, arg_list[0]))\n",
    "        return points,\n",
    "\n",
    "    def animate(i):\n",
    "        points.set_offsets(frame_data[i])\n",
    "        if (i + 15) % 30 == 0:\n",
    "            title.set_text('UMAP with {}={}'.format(arg_name, arg_list[(i + 15) // 30]))\n",
    "        return points,\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(frame_data), interval=20, blit=True)\n",
    "    anim.save('umap_anim-{}.gif'.format(arg_name), writer='imagemagick', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T10:04:27.929611Z",
     "start_time": "2019-02-22T10:04:27.922796Z"
    }
   },
   "outputs": [],
   "source": [
    "def animate_param(data, arg_name='n_neighbors', arg_list=[3,4,5,7,9,11,15,20,25,30,40,60,100]):\n",
    "    frame_data = generate_frame_data(data, arg_name, arg_list)\n",
    "    create_animation(frame_data, arg_name, arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T10:15:59.213120Z",
     "start_time": "2019-02-22T10:15:57.356061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imagemagick\n",
      "\u001b[31m  Could not find a version that satisfies the requirement imagemagick (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for imagemagick\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#import ffmpeg \n",
    "!pip3 install imagemagick\n",
    "#animate_param(data, 'n_neighbors', [3,4,5,7,10,15])\n",
    "#animate_param(data, 'min_dist', [0.0, 0.01, 0.1, 0.2, 0.4, 0.6, 0.9])\n",
    "#animate_param(data, 'gamma', [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0])\n",
    "#animate_param(data, 'local_connectivity', [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0])\n",
    "#animate_param(data, 'bandwidth', [1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0])\n",
    "#animate_param(data, 'set_op_mix_ratio', np.linspace(0.0, 1.0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"posw.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data['RFPlog']=np.log2(data['RFP'])\n",
    "data.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "DNAortho = ('A','1000') , ('T','0100') ,  ('G','0010'), ('C','0001')\n",
    "data['DNASeqOrtho']=reduce(lambda a, kv: a.str.replace(*kv), DNAortho, data['DNASeq'])\n",
    "data['DNASeqOrtho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Variable(torch.Tensor(data['DNASeqOrtho'].apply(lambda x: float(x.find('10'))).reshape(-1,1))).cuda()\n",
    "y=Variable(torch.Tensor(data['RFPlog'].reshape(-1,1))).cuda()\n",
    "X, y\n",
    "#int(data['DNASeqOrtho'].sum(),2)\n",
    "#data['DNASeqOrtho'].apply(lambda x: x.find('1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__() \n",
    "        self.linear = nn.Linear(input_dim, output_dim,bias=True).cuda()\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "l_rate = 0.01\n",
    "model = LinearRegressionModel(input_dim,output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr = l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch +=1\n",
    "    optimiser.zero_grad()\n",
    "    outputs = model.forward(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()# back props\n",
    "    optimiser.step()# update the parameters\n",
    "    print('epoch {}, loss {}'.format(epoch,loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted =model.forward(X).cpu().data.numpy()\n",
    "#plt.plot(X, y, 'i', label = 'from data', alpha = .5)\n",
    "plt.plot(y.cpu().data.numpy(), predicted, label = 'prediction', alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#sphx-glr-beginner-blitz-data-parallel-tutorial-py\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "input_size = 5\n",
    "output_size = 2\n",
    "batch_size = 30\n",
    "data_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset(input_size, 100),\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    # Our model\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.fc(input)\n",
    "        print(\"  In Model: input size\", input.size(),\n",
    "              \"output size\", output.size())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size, output_size)\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in rand_loader:\n",
    "    if torch.cuda.is_available():\n",
    "        input_var = Variable(data.cuda())\n",
    "    else:\n",
    "        input_var = Variable(data)\n",
    "\n",
    "    output = model(input_var)\n",
    "    print(\"Outside: input size\", input_var.size(),\n",
    "          \"output_size\", output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?time_continue=96&v=vMZ7tK-RYYc\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from numba import vectorize, cuda\n",
    "\n",
    "#@vectorize(['float32(float32,float32)'],target='cuda')\n",
    "def subVector(ε,σ):\n",
    "    return ε + σ\n",
    "\n",
    "Elements=10000\n",
    "A=np.ones(Elements,dtype=np.float32)\n",
    "B=np.ones(Elements,dtype=np.float32)\n",
    "C=subVector(A,B)\n",
    "ts=time.time()\n",
    "te=time.time()\n",
    "print(C,te-ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI\n",
    "from pyro.optim import Adam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
