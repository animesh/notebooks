{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 2) / 2]+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.{SparkConf}\n",
    "\n",
    "val conf = new SparkConf()\n",
    "conf.setAppName(\"Error tolerant search peaks detection\")\n",
    "kernel.createSparkContext(conf)\n",
    "val people = spark.read.json(\"/opt/datasets/people.json\")\n",
    "people.registerTempTable(\"people\")\n",
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
=======
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[154] at parallelize at <console>:163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: Unknown\n",
       "Message: Unable to retrieve error!\n",
       "StackTrace: "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 6ad2e5596663d633582af862423b2908d042f78e
   "source": [
    "import org.apache.spark.mllib.clustering.{GaussianMixture, GaussianMixtureModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.random.RandomRDDs._\n",
    "val data = sc.parallelize(\n",
    "  Seq(\n",
    "    Vectors.dense(1.0, 10.0, 100.0),\n",
    "    Vectors.dense(2.0, 20.0, 200.0),\n",
    "    Vectors.dense(3.0, 30.0, 300.0)\n",
    "  )\n",
    ")\n",
    "println(data)\n",
    "\n",
    "//val parsedData=poissonRDD(sc, 1, 10000, seed=1)\n",
    "val parsedData = data\n",
    "\n",
    "// Cluster the data into two classes using GaussianMixture\n",
    "val gmm = new GaussianMixture().setK(2).run(parsedData)\n",
    "\n",
    "// Save and load model\n",
    "gmm.save(sc, \"target/org/apache/spark/GaussianMixtureExample/GaussianMixtureModel\")\n",
    "val sameModel = GaussianMixtureModel.load(sc,\n",
    "  \"target/org/apache/spark/GaussianMixtureExample/GaussianMixtureModel\")\n",
    "\n",
    "// output parameters of max-likelihood model\n",
    "for (i <- 0 until gmm.k) {\n",
    "  println(\"weight=%f\\nmu=%s\\nsigma=\\n%s\\n\" format\n",
    "    (gmm.weights(i), gmm.gaussians(i).mu, gmm.gaussians(i).sigma))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required package: methods\n",
      "\n",
      "Attaching package: 'SparkR'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cov, filter, lag, na.omit, predict, sd, var, window\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,\n",
      "    rank, rbind, sample, startsWith, subset, summary, transform, union\n",
      "\n",
      "Warning message:\n",
      "In rm(\".sparkRcon\", envir = .sparkREnv) : object '.sparkRcon' not found\n",
      "[1] \"ExistingPort:\" \"44481\"        \n",
      "Error in value[[3L]](cond) : \n",
      "  Failed to connect JVM: Error in socketConnection(host = hostname, port = port, server = FALSE, : argument \"timeout\" is missing, with no default\n",
      "Calls: sparkR.connect ... tryCatch -> tryCatchList -> tryCatchOne -> <Anonymous>\n",
      "Execution halted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Left(Map(text/plain -> Magic SparkR failed to execute with error:\n",
       "null was reset!))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required package: methods\n",
      "\n",
      "Attaching package: 'SparkR'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cov, filter, lag, na.omit, predict, sd, var, window\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,\n",
      "    rank, rbind, sample, startsWith, subset, summary, transform, union\n",
      "\n",
      "Warning message:\n",
      "In rm(\".sparkRcon\", envir = .sparkREnv) : object '.sparkRcon' not found\n",
      "[1] \"ExistingPort:\" \"44481\"        \n",
      "Error in value[[3L]](cond) : \n",
      "  Failed to connect JVM: Error in socketConnection(host = hostname, port = port, server = FALSE, : argument \"timeout\" is missing, with no default\n",
      "Calls: sparkR.connect ... tryCatch -> tryCatchList -> tryCatchOne -> <Anonymous>\n",
      "Execution halted\n"
     ]
    }
   ],
>>>>>>> 6ad2e5596663d633582af862423b2908d042f78e
   "source": [
    "%%SparkR\n",
    "//library(\"mclust\")\n",
    "plot(rand(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required package: methods\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Left(Map(text/plain -> Magic PySpark failed to execute with error:\n",
       "Py4JJavaError: An error occurred while calling o1.javaSparkContext.\n",
       ": java.lang.NullPointerException\n",
       "\tat org.apache.toree.kernel.api.Kernel._sparkContext(Kernel.scala:92)\n",
       "\tat org.apache.toree.kernel.api.Kernel._javaSparkContext(Kernel.scala:93)\n",
       "\tat org.apache.toree.kernel.api.Kernel.javaSparkContext(Kernel.scala:406)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodIn..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'SparkR'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cov, filter, lag, na.omit, predict, sd, var, window\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,\n",
      "    rank, rbind, sample, startsWith, subset, summary, transform, union\n",
      "\n",
      "Warning message:\n",
      "In rm(\".sparkRcon\", envir = .sparkREnv) : object '.sparkRcon' not found\n",
      "[1] \"ExistingPort:\" \"44481\"        \n",
      "Error in value[[3L]](cond) : \n",
      "  Failed to connect JVM: Error in socketConnection(host = hostname, port = port, server = FALSE, : argument \"timeout\" is missing, with no default\n",
      "Calls: sparkR.connect ... tryCatch -> tryCatchList -> tryCatchOne -> <Anonymous>\n",
      "Execution halted\n"
     ]
    }
   ],
   "source": [
    "%%PySpark\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "output_notebook()\n",
    "p = figure()\n",
    "p.line([1,2,3,4,5],[6,7,8,9,10], line_width=2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Left(Map(text/plain -> Magic PySpark failed to execute with error:\n",
       "null))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%PySpark\n",
    "cars = spark.read.json(\"/opt/cars.json\")\n",
    "cars.registerTempTable(\"cars\")\n",
    "cars.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON::: Starting imports\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/kernel-PySpark-f25ee4fd-1311-45d1-b8fc-fae45b88f7de/pyspark_runner.py\", line 21, in <module>\n",
      "    from py4j.java_gateway import java_import, JavaGateway, GatewayClient\n",
      "ImportError: No module named 'py4j'\n",
      "Loading required package: methods\n",
      "PYTHON::: Starting imports\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/kernel-PySpark-f25ee4fd-1311-45d1-b8fc-fae45b88f7de/pyspark_runner.py\", line 21, in <module>\n",
      "    from py4j.java_gateway import java_import, JavaGateway, GatewayClient\n",
      "ImportError: No module named 'py4j'\n",
      "PYTHON::: Starting imports\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/kernel-PySpark-f25ee4fd-1311-45d1-b8fc-fae45b88f7de/pyspark_runner.py\", line 21, in <module>\n",
      "    from py4j.java_gateway import java_import, JavaGateway, GatewayClient\n",
      "ImportError: No module named 'py4j'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Left(Map(text/plain -> Magic sql failed to execute with error:\n",
       "Table or view not found: cars; line 1 pos 14))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON::: Starting imports\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/kernel-PySpark-f25ee4fd-1311-45d1-b8fc-fae45b88f7de/pyspark_runner.py\", line 21, in <module>\n",
      "    from py4j.java_gateway import java_import, JavaGateway, GatewayClient\n",
      "ImportError: No module named 'py4j'\n",
      "\n",
      "Attaching package: 'SparkR'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cov, filter, lag, na.omit, predict, sd, var, window\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,\n",
      "    rank, rbind, sample, startsWith, subset, summary, transform, union\n",
      "\n",
      "Warning message:\n",
      "In rm(\".sparkRcon\", envir = .sparkREnv) : object '.sparkRcon' not found\n",
      "[1] \"ExistingPort:\" \"41881\"        \n",
      "Error in value[[3L]](cond) : \n",
      "  Failed to connect JVM: Error in socketConnection(host = hostname, port = port, server = FALSE, : argument \"timeout\" is missing, with no default\n",
      "Calls: sparkR.connect ... tryCatch -> tryCatchList -> tryCatchOne -> <Anonymous>\n",
      "Execution halted\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "select * from cars where manufacturer == 'Audi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt <- \"/data/promec/testR/b1947_293T_proteinID_11B_QE3_122212.pep.xml\"\n",
    "tttt <- pepXML2tab(tt)\n",
    "tttt[1,]\n",
    "hist(as.numeric(tttt$massdiff))\n",
    "passed <- PSMfilter(tttt, pepFDR = 0.01, scorecolumn = 'spscore', hitrank = 1,minpeplen = 6, decoysuffix = '_REVERSED')\n",
    "passed[1, ]\n",
    "mdiff = as.numeric(passed$precursor_neutral_mass) - as.numeric(passed$calc_neutral_pep_mass)\n",
    "dhist = hist(mdiff)\n",
    "warnings()\n",
    "hist(log10(dhist$density))\n",
    "max(log10(dhist$density))\n",
    "mod4 = densityMclust(mdiff)\n",
    "mod4$parameters$mean\n",
    "summary(mod4)\n",
    "plot(mod4, what = \"BIC\")\n",
    "plot(mod4, what = \"density\")\n",
    "plot(mod4, what = \"diagnostic\", type = \"qq\")\n",
    "wtf = paste0(tt,\"mu.txt\")\n",
    "write.table(mod4$parameters, wtf,sep = \"\\t\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%SparkR\n",
    "plot(mod4, what = \"BIC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
