{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://github.com/google/nucleus/issues/18#issuecomment-500861154\n",
    "#sudo python3 -m pip install tensorflow==2.0.0-alpha0\n",
    "#sudo python3 -m pip install google-nucleus==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/animeshs/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python deepvariant/bin/make_examples.zip --mode calling  --ref deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions \"chr20:10,000,000-10,010,000\" --examples deepvariant/quickstart-output/examples.tfrecord.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python deepvariant/bin/call_variants.zip --outfile deepvariant/quickstart-output/examples.tfrecord.cvo.gz --examples deepvariant/quickstart-output/examples.tfrecord.gz --checkpoint deepvariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python deepvariant/bin/postprocess_variants.zip --ref deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --infile  deepvariant/quickstart-output/examples.tfrecord.cvo.gz --outfile  deepvariant/quickstart-output/examples.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!$HOME/hap.py-install/bin/hap.py deepvariant/quickstart-testdata/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz deepvariant/quickstart-output/examples.vcf -f deepvariant/quickstart-testdata/test_nist.b37_chr20_100kbp_at_10mb.bed -r deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta -o happyeg.out --engine=vcfeval -l chr20:10000000-10010000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://colab.research.google.com/github/google/nucleus/blob/master/nucleus/examples/dna_sequencing_error_correction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/google/nucleus/issues/15#issuecomment-498851344\n",
    "from nucleus.io import fasta\n",
    "from nucleus.io import sam\n",
    "from nucleus.io import vcf\n",
    "from nucleus.io.genomics_writer import TFRecordWriter\n",
    "from nucleus.protos import reads_pb2\n",
    "from nucleus.util import cigar\n",
    "from nucleus.util import ranges\n",
    "from nucleus.util import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir=\"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "_ALLOWED_CIGAR_OPS = frozenset([cigar.CHAR_TO_CIGAR_OPS[op] for op in 'MX='])\n",
    "_ALLOWED_BASES = 'ACGT'\n",
    "_TRAIN = 'train.tfrecord'\n",
    "_EVAL = 'eval.tfrecord'\n",
    "_TEST = 'test.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hparams):\n",
    "  \"\"\"Convolutional neural network architecture.\"\"\"\n",
    "\n",
    "  l2_reg = tf.keras.regularizers.l2\n",
    "\n",
    "  return tf.keras.models.Sequential([\n",
    "\n",
    "      # Two convolution + maxpooling blocks\n",
    "      layers.Conv1D(\n",
    "          filters=16,\n",
    "          kernel_size=5,\n",
    "          activation=tf.nn.relu,\n",
    "          kernel_regularizer=l2_reg(hparams.l2)),\n",
    "      layers.MaxPool1D(pool_size=3, strides=1),\n",
    "      layers.Conv1D(\n",
    "          filters=16,\n",
    "          kernel_size=3,\n",
    "          activation=tf.nn.relu,\n",
    "          kernel_regularizer=l2_reg(hparams.l2)),\n",
    "      layers.MaxPool1D(pool_size=3, strides=1),\n",
    "\n",
    "      # Flatten the input volume\n",
    "      layers.Flatten(),\n",
    "\n",
    "      # Two fully connected layers, each followed by a dropout layer\n",
    "      layers.Dense(\n",
    "          units=16,\n",
    "          activation=tf.nn.relu,\n",
    "          kernel_regularizer=l2_reg(hparams.l2)),\n",
    "      layers.Dropout(rate=0.3),\n",
    "      layers.Dense(\n",
    "          units=16,\n",
    "          activation=tf.nn.relu,\n",
    "          kernel_regularizer=l2_reg(hparams.l2)),\n",
    "      layers.Dropout(rate=0.3),\n",
    "\n",
    "      # Output layer with softmax activation\n",
    "      layers.Dense(units=len(_ALLOWED_BASES), activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfrecord_datasets(hparams):\n",
    "  \"\"\"Writes out TFRecords files for training, evaluation, and test datasets.\"\"\"\n",
    "  if not os.path.exists(hparams.out_dir):\n",
    "    os.makedirs(hparams.out_dir)\n",
    "\n",
    "  # Fraction of examples in each dataset.\n",
    "  train_eval_test_split = [0.7, 0.2, 0.1]\n",
    "  num_train_examples = 0\n",
    "  num_eval_examples = 0\n",
    "  num_test_examples = 0\n",
    "\n",
    "  # Generate training, test, and evaluation examples.\n",
    "  with TFRecordWriter(os.path.join(hparams.out_dir, _TRAIN)) as train_out, \\\n",
    "       TFRecordWriter(os.path.join(hparams.out_dir, _EVAL)) as eval_out, \\\n",
    "       TFRecordWriter(os.path.join(hparams.out_dir, _TEST)) as test_out:\n",
    "    all_examples = make_ngs_examples(hparams)\n",
    "    for example in all_examples:\n",
    "      r = random.random()\n",
    "      if r < train_eval_test_split[0]:\n",
    "        train_out.write(proto=example)\n",
    "        num_train_examples += 1\n",
    "      elif r < train_eval_test_split[0] + train_eval_test_split[1]:\n",
    "        eval_out.write(proto=example)\n",
    "        num_eval_examples += 1\n",
    "      else:\n",
    "        test_out.write(proto=example)\n",
    "        num_test_examples += 1\n",
    "  print('# of training examples: %d' % num_train_examples)\n",
    "  print('# of evaluation examples: %d' % num_eval_examples)\n",
    "  print('# of test examples: %d' % num_test_examples)\n",
    "\n",
    "\n",
    "def make_ngs_examples(hparams):\n",
    "  \"\"\"Generator function that yields training, evaluation and test examples.\"\"\"\n",
    "  ref_reader = fasta.IndexedFastaReader(input_path=hparams.ref_path)\n",
    "  vcf_reader = vcf.VcfReader(input_path=hparams.vcf_path)\n",
    "  read_requirements = reads_pb2.ReadRequirements()\n",
    "  sam_reader = sam.SamReader(\n",
    "      input_path=hparams.bam_path, read_requirements=read_requirements)\n",
    "\n",
    "  # Use a separate SAM reader to query for reads falling in the pileup range.\n",
    "  sam_query_reader = sam.SamReader(\n",
    "      input_path=hparams.bam_path, read_requirements=read_requirements)\n",
    "  used_pileup_ranges = set()\n",
    "  with ref_reader, vcf_reader, sam_reader, sam_query_reader:\n",
    "    for read in sam_reader:\n",
    "\n",
    "      # Check that read has cigar string present and allowed alignment.\n",
    "      if not read.alignment.cigar:\n",
    "        print('Skipping read, no cigar alignment found')\n",
    "        continue\n",
    "      if not has_allowed_alignment(read):\n",
    "        continue\n",
    "\n",
    "      # Obtain window that will be used to construct an example.\n",
    "      read_range = utils.read_range(read)\n",
    "      ref = ref_reader.query(region=read_range)\n",
    "      pileup_range = get_pileup_range(hparams, read, read_range, ref)\n",
    "\n",
    "      # Do not construct multiple examples with the same pileup range.\n",
    "      pileup_range_serialized = pileup_range.SerializeToString()\n",
    "      if pileup_range_serialized in used_pileup_ranges:\n",
    "        continue\n",
    "      used_pileup_ranges.add(pileup_range_serialized)\n",
    "\n",
    "      # Get reference sequence, reads, and truth variants for the pileup range.\n",
    "      pileup_reads = list(sam_query_reader.query(region=pileup_range))\n",
    "      pileup_ref = ref_reader.query(region=pileup_range)\n",
    "      pileup_variants = list(vcf_reader.query(region=pileup_range))\n",
    "      if is_usable_example(pileup_reads, pileup_variants, pileup_ref):\n",
    "        yield make_example(hparams, pileup_reads, pileup_ref, pileup_range)\n",
    "\n",
    "\n",
    "def get_pileup_range(hparams, read, read_range, ref):\n",
    "  \"\"\"Returns a range that will be used to construct one example.\"\"\"\n",
    "\n",
    "  # Find error positions where read and reference differ.\n",
    "  ngs_read_length = read_range.end - read_range.start\n",
    "  error_indices = [\n",
    "      i for i in range(ngs_read_length) if ref[i] != read.aligned_sequence[i]\n",
    "  ]\n",
    "\n",
    "  # If read and reference sequence are the same, create an example centered\n",
    "  # at middle base of read.\n",
    "  if not error_indices:\n",
    "    error_idx = ngs_read_length // 2\n",
    "\n",
    "  # If read and reference differ at one or more positions, create example\n",
    "  # centered at a random error position.\n",
    "  else:\n",
    "    error_idx = random.choice(error_indices)\n",
    "\n",
    "  error_pos = read_range.start + error_idx\n",
    "  flank_size = hparams.window_size // 2\n",
    "  return ranges.make_range(\n",
    "      chrom=read_range.reference_name,\n",
    "      start=error_pos - flank_size,\n",
    "      end=error_pos + flank_size + 1)\n",
    "\n",
    "\n",
    "def has_allowed_alignment(read):\n",
    "  \"\"\"Determines whether a read's CIGAR string has the allowed alignments.\"\"\"\n",
    "  return all([c.operation in _ALLOWED_CIGAR_OPS for c in read.alignment.cigar])\n",
    "\n",
    "\n",
    "def is_usable_example(reads, variants, ref_bases):\n",
    "  \"\"\"Determines whether a particular reference region and read can be used.\"\"\"\n",
    "  # Discard examples with variants or no mapped reads.\n",
    "  if variants or not reads:\n",
    "    return False\n",
    "\n",
    "  # Use only examples where all reads have simple alignment and allowed bases.\n",
    "  for read in reads:\n",
    "    if not has_allowed_alignment(read):\n",
    "      return False\n",
    "    if any(base not in _ALLOWED_BASES for base in read.aligned_sequence):\n",
    "      return False\n",
    "\n",
    "  # Reference should only contain allowed bases.\n",
    "  if any(base not in _ALLOWED_BASES for base in ref_bases):\n",
    "    return False\n",
    "  return True\n",
    "\n",
    "\n",
    "def make_example(hparams, pileup_reads, pileup_ref, pileup_range):\n",
    "  \"\"\"Takes in an input sequence and outputs tf.train.Example ProtocolMessages.\n",
    "\n",
    "  Each example contains the following features: A counts, C counts, G counts,\n",
    "  T counts, reference sequence, correct base label.\n",
    "  \"\"\"\n",
    "  assert len(pileup_ref) == hparams.window_size\n",
    "  example = tf.train.Example()\n",
    "  base_counts = np.zeros(shape=[hparams.window_size, len(_ALLOWED_BASES)])\n",
    "\n",
    "  for read in pileup_reads:\n",
    "    read_position = read.alignment.position.position\n",
    "    read_ints = [_ALLOWED_BASES.index(b) for b in read.aligned_sequence]\n",
    "    one_hot_read = np.zeros((len(read_ints), len(_ALLOWED_BASES)))\n",
    "    one_hot_read[np.arange(len(one_hot_read)), read_ints] = 1\n",
    "\n",
    "    window_start = read_position - pileup_range.start\n",
    "    window_end = window_start + len(read_ints)\n",
    "\n",
    "    # If read falls outside of window, adjust start/end indices for window.\n",
    "    window_start = max(0, window_start)\n",
    "    window_end = min(window_end, hparams.window_size)\n",
    "\n",
    "    # We consider four possible scenarios for each read and adjust start/end\n",
    "    # indices to only include portions of read that overlap the window.\n",
    "    # 1) Read extends past 5' end of window\n",
    "    # 2) Read extends past 3' end of window\n",
    "    # 3) Read extends past 5' and 3' ends of window\n",
    "    # 4) Read falls entirely within window\n",
    "    if window_start == 0 and window_end != hparams.window_size:\n",
    "      read_start = pileup_range.start - read_position\n",
    "      read_end = None\n",
    "    if window_end == hparams.window_size and window_start != 0:\n",
    "      read_start = None\n",
    "      read_end = -1 * ((read_position + len(read_ints)) - pileup_range.end)\n",
    "    if window_start == 0 and window_end == hparams.window_size:\n",
    "      read_start = pileup_range.start - read_position\n",
    "      read_end = read_start + hparams.window_size\n",
    "    if window_start != 0 and window_end != hparams.window_size:\n",
    "      read_start = None\n",
    "      read_end = None\n",
    "    base_counts[window_start:window_end] += one_hot_read[read_start:read_end]\n",
    "\n",
    "  # Use fractions at each position instead of raw base counts.\n",
    "  base_counts /= np.expand_dims(np.sum(base_counts, axis=-1), -1)\n",
    "\n",
    "  # Save counts/fractions for each base separately.\n",
    "  features = example.features\n",
    "  for i in range(len(_ALLOWED_BASES)):\n",
    "    key = '%s_counts' % _ALLOWED_BASES[i]\n",
    "    features.feature[key].float_list.value.extend(list(base_counts[:, i]))\n",
    "\n",
    "  features.feature['ref_sequence'].int64_list.value.extend(\n",
    "      [_ALLOWED_BASES.index(base) for base in pileup_ref])\n",
    "  flank_size = hparams.window_size // 2\n",
    "  true_base = pileup_ref[flank_size]\n",
    "  features.feature['label'].int64_list.value.append(\n",
    "      _ALLOWED_BASES.index(true_base))\n",
    "\n",
    "  return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(hparams, filename, num_epochs):\n",
    "  \"\"\"Reads in and processes the TFRecords dataset.\n",
    "\n",
    "  Builds a pipeline that returns pairs of features, label.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define field names, types, and sizes for TFRecords.\n",
    "  proto_features = {\n",
    "      'A_counts':\n",
    "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
    "      'C_counts':\n",
    "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
    "      'G_counts':\n",
    "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
    "      'T_counts':\n",
    "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
    "      'ref_sequence':\n",
    "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.int64),\n",
    "      'label':\n",
    "          tf.io.FixedLenFeature(shape=[1], dtype=tf.int64),\n",
    "  }\n",
    "\n",
    "  def _process_input(proto_string):\n",
    "    \"\"\"Helper function for input function that parses a serialized example.\"\"\"\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(\n",
    "        serialized=proto_string, features=proto_features)\n",
    "\n",
    "    # Stack counts/fractions for all bases to create input of dimensions\n",
    "    # `hparams.window_size` x len(_ALLOWED_BASES).\n",
    "    feature_columns = []\n",
    "    for base in _ALLOWED_BASES:\n",
    "      feature_columns.append(parsed_features['%s_counts' % base])\n",
    "    features = tf.stack(feature_columns, axis=-1)\n",
    "    label = parsed_features['label']\n",
    "    return features, label\n",
    "\n",
    "  ds = tf.data.TFRecordDataset(filenames=filename)\n",
    "  ds = ds.map(map_func=_process_input)\n",
    "  ds = ds.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "  ds = ds.batch(batch_size=hparams.batch_size).repeat(count=num_epochs)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(hparams, use_existing_data=False, seed=1):\n",
    "  \"\"\"Creates a model, runs training and evaluation.\"\"\"\n",
    "\n",
    "  # Set seed for reproducibility.\n",
    "  random.seed(seed)\n",
    "  tf.random.set_seed(seed)\n",
    "\n",
    "  if not use_existing_data:\n",
    "    print('Generating data...')\n",
    "    generate_tfrecord_datasets(hparams)\n",
    "\n",
    "  train_dataset = get_dataset(\n",
    "      hparams, filename=os.path.join(hparams.out_dir, _TRAIN), num_epochs=1)\n",
    "  eval_dataset = get_dataset(\n",
    "      hparams, filename=os.path.join(hparams.out_dir, _EVAL), num_epochs=1)\n",
    "  test_dataset = get_dataset(\n",
    "      hparams, filename=os.path.join(hparams.out_dir, _TEST), num_epochs=1)\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(lr=hparams.learning_rate)\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      hparams.log_dir, histogram_freq=1)\n",
    "  model = build_model(hparams)\n",
    "  model.compile(\n",
    "      optimizer=optimizer,\n",
    "      loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "  print('Training the model. This should take ~6 minutes...')\n",
    "  model.fit_generator(\n",
    "      train_dataset,\n",
    "      epochs=hparams.total_epochs,\n",
    "      validation_data=eval_dataset,\n",
    "      callbacks=[tensorboard_callback],\n",
    "      verbose=0)\n",
    "  print('Training complete. Obtaining final metrics...')\n",
    "  eval_metrics = model.evaluate(eval_dataset, verbose=0)\n",
    "  test_metrics = model.evaluate(test_dataset, verbose=0)\n",
    "  print('Final eval metrics - loss: %f - accuracy: %f' %\n",
    "        (eval_metrics[0], eval_metrics[1]))\n",
    "  print('Final test metrics - loss: %f - accuracy: %f' %\n",
    "        (test_metrics[0], test_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseHparams(object):\n",
    "  \"\"\"Default hyperparameters.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               total_epochs=100,\n",
    "               learning_rate=0.004,\n",
    "               l2=0.001,\n",
    "               batch_size=256,\n",
    "               window_size=21,\n",
    "               ref_path='hs37d5.fa.gz',\n",
    "               vcf_path='NA12878_calls.vcf.gz',\n",
    "               bam_path='NA12878_sliced.bam',\n",
    "               out_dir='examples',\n",
    "               model_dir='ngs_model',\n",
    "               log_dir=log_dir):\n",
    "\n",
    "    self.total_epochs = total_epochs\n",
    "    self.learning_rate = learning_rate\n",
    "    self.l2 = l2\n",
    "    self.batch_size = batch_size\n",
    "    self.window_size = window_size\n",
    "    self.ref_path = ref_path\n",
    "    self.vcf_path = vcf_path\n",
    "    self.bam_path = bam_path\n",
    "    self.out_dir = out_dir\n",
    "    self.model_dir = model_dir\n",
    "    self.log_dir = log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "# of training examples: 7152\n",
      "# of evaluation examples: 1993\n",
      "# of test examples: 1032\n",
      "Training the model. This should take ~6 minutes...\n",
      "Training complete. Obtaining final metrics...\n",
      "Final eval metrics - loss: 0.086099 - accuracy: 0.992975\n",
      "Final test metrics - loss: 0.089156 - accuracy: 0.995155\n"
     ]
    }
   ],
   "source": [
    "hparams = BaseHparams()\n",
    "run(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "wget https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NIST_NA12878_HG001_HiSeq_300x/RMNISTHS_30xdownsample.bam\n",
    "wget https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz\n",
    "wget https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz.tbi\n",
    "wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz\n",
    "wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz.fai\n",
    "wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz.gzi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "samtools index $HOME/HeLa/RMNISTHS_30xdownsample.bam\n",
    "samtools view -h $HOME/HeLa/RMNISTHS_30xdownsample.bam 20:10,000,000-10,100,000  -o NA12878_sliced.bam\n",
    "samtools index NA12878_sliced.bam\n",
    "samtools view NA12878_sliced.bam\n",
    "\n",
    "cp HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz NA12878_calls.vcf.gz\n",
    "cp HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz.tbi NA12878_calls.vcf.gz.tbi\n",
    "\n",
    "ln -s $HOME/HeLa/hs37d5.fa.gz* .\n",
    "ln -s $HOME/HeLa/NA12878_sliced.bam* .\n",
    "ln -s $HOME/HeLa/NA12878_calls.vcf.gz* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#http://localhost:6006/#graphs&run=20191002-110409%2Ftrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
