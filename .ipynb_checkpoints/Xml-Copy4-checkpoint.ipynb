{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import concat, col, lit, regexp_replace, monotonically_increasing_id, unix_timestamp, from_unixtime\n",
    "#from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName(\"pepXMLtoJSON\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\").set(\"spark.executor.cores\", \"3\").set(\"spark.cores.max\", \"12\")\n",
    "conf.set(\"spark.jars.packages\", \"com.databricks:spark-xml_2.11:0.4.1\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 -rw-r--r--. 1 root root   0 Nov 14 01:15 /data/promec/new/mzML/140122_bsa.pep.xml\n",
      "71M -rw-r--r--. 1 root root 71M Nov 12 23:41 /data/promec/new/mzML/140331_bsa.pep.xml\n",
      "15M -rw-r--r--. 1 root root 15M Nov 14 01:15 /data/promec/new/mzML/140428_bsa.pep.xml\n",
      "26M -rw-r--r--. 1 root root 26M Nov 14 01:15 /data/promec/new/mzML/141210_bsa.pep.xml\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls -aslh /data/promec/new/mzML/14*_bsa.pep.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inpf='/data/promec/new/mzML/140331_bsa.pep.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "F1 = udf(lambda x: '-1' if x in not_found_cat else x, StringType())\n",
    "k = test.withColumn(\"NEW_Product_ID\",F1(test[\"Product_ID\"])).select('NEW_Product_ID')\n",
    "diff_cat_in_train_test=k.select('NEW_Product_ID').subtract(train.select('Product_ID'))\n",
    "diff_cat_in_train_test.distinct().count()# For distinct count\n",
    "diff_cat_in_train_test.distinct().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "    .option('attributePrefix','xxx')\\\n",
    "    .options(rowTag='spectrum_query')\\\n",
    "    .load(inpf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-11-05T08:40:36\n"
     ]
    }
   ],
   "source": [
    "dfhdr = sqlContext.read.format('com.databricks.spark.xml').options(rowTag='msms_pipeline_analysis', valueTag='date').load(inpf)\n",
    "dfhdr.schema\n",
    "pepxmltime=dfhdr.select(\"_date\").rdd.flatMap(list).first()\n",
    "#dt=dfhdr.select(\"_date\").show()pwd\n",
    "print(pepxmltime)\n",
    "#2016-10-26T05:10:17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4911.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 61.0 failed 4 times, most recent failure: Lost task 0.3 in stage 61.0 (TID 144, 192.168.83.91, executor 18): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in <lambda>\n    mapper = lambda a: udf(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-291-28fd90c45a80>\", line 12, in <lambda>\nNameError: name 'datetime' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in <lambda>\n    mapper = lambda a: udf(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-291-28fd90c45a80>\", line 12, in <lambda>\nNameError: name 'datetime' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-28fd90c45a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4911.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 61.0 failed 4 times, most recent failure: Lost task 0.3 in stage 61.0 (TID 144, 192.168.83.91, executor 18): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in <lambda>\n    mapper = lambda a: udf(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-291-28fd90c45a80>\", line 12, in <lambda>\nNameError: name 'datetime' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in <lambda>\n    mapper = lambda a: udf(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-291-28fd90c45a80>\", line 12, in <lambda>\nNameError: name 'datetime' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "timestr = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "#timestr = time.strptime\n",
    "#df.schema\n",
    "#lit(pepxmltime)\n",
    "\n",
    "func =  udf (lambda x: datetime.strptime(x, '%M/%d/%Y'), DateType())\n",
    "df1 = sqlContext.createDataFrame([(\"11/25/1991\",\"11/24/1991\",\"11/30/1991\"), \n",
    "                            (\"11/25/1391\",\"11/24/1992\",\"11/30/1992\")], schema=['first', 'second', 'third'])\n",
    "\n",
    "# Setting an user define function:\n",
    "# This function converts the string cell into a date:\n",
    "func =  udf (lambda x: datetime.strptime(x, '%M/%d/%Y'), DateType())\n",
    "\n",
    "df = df1.withColumn('test', func(col('first')))\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#timestr = time.total_second\n",
    "vd=from_unixtime(pepxmltime,format=timeFmt).cast(DateType())\n",
    "timeFmt = \"yyyy-MM-dd'T'HH:mm:ss.SSS\"\n",
    "timeDiff = (unix_timestamp(pepxmltime, format=timeFmt)\n",
    "            - unix_timestamp(pepxmltime, format=timeFmt))\n",
    "#df = df.withColumn(\"Duration\", timeDiff)\n",
    "#timeDiff\n",
    "vd\n",
    "timestr = time.strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dform=df.withColumn('_index', concat(lit('promec')))\n",
    "#dform=dform.withColumn('_type', concat(lit('search_hit')))\n",
    "dform=df.withColumn('@timestamp', lit(pepxmltime))\n",
    "#dform=dform.withColumn('_id', regexp_replace(concat(df.spectrum,lit(timestr)),\"[^\\d]+\", \"\"))\n",
    "#dform=dform.withColumn('@timestamp', concat(lit(timestr),monotonically_increasing_id()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`20170223172550`' given input columns: [xxxspectrum, xxxindex, xxxretention_time_sec, search_result, xxxend_scan, xxxprecursor_neutral_mass, xxxstart_scan, xxxassumed_charge, xxxspectrumNativeID];;\\n'Project [search_result#982, xxxassumed_charge#983L, xxxend_scan#984L, xxxindex#985L, xxxprecursor_neutral_mass#986, xxxretention_time_sec#987, xxxspectrum#988, xxxspectrumNativeID#989, xxxstart_scan#990L, unix_timestamp('20170223172550, yyyy-MM-dd'T'HH:mm:ss.SSS) AS @tttimestamp#1300]\\n+- Repartition 1, true\\n   +- Relation[search_result#982,xxxassumed_charge#983L,xxxend_scan#984L,xxxindex#985L,xxxprecursor_neutral_mass#986,xxxretention_time_sec#987,xxxspectrum#988,xxxspectrumNativeID#989,xxxstart_scan#990L] XmlRelation(<function0>,Some(/data/promec/new/mzML/140331_bsa.pep.xml),Map(attributeprefix -> xxx, rowtag -> spectrum_query, path -> /data/promec/new/mzML/140331_bsa.pep.xml),null)\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1592.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`20170223172550`' given input columns: [xxxspectrum, xxxindex, xxxretention_time_sec, search_result, xxxend_scan, xxxprecursor_neutral_mass, xxxstart_scan, xxxassumed_charge, xxxspectrumNativeID];;\n'Project [search_result#982, xxxassumed_charge#983L, xxxend_scan#984L, xxxindex#985L, xxxprecursor_neutral_mass#986, xxxretention_time_sec#987, xxxspectrum#988, xxxspectrumNativeID#989, xxxstart_scan#990L, unix_timestamp('20170223172550, yyyy-MM-dd'T'HH:mm:ss.SSS) AS @tttimestamp#1300]\n+- Repartition 1, true\n   +- Relation[search_result#982,xxxassumed_charge#983L,xxxend_scan#984L,xxxindex#985L,xxxprecursor_neutral_mass#986,xxxretention_time_sec#987,xxxspectrum#988,xxxspectrumNativeID#989,xxxstart_scan#990L] XmlRelation(<function0>,Some(/data/promec/new/mzML/140331_bsa.pep.xml),Map(attributeprefix -> xxx, rowtag -> spectrum_query, path -> /data/promec/new/mzML/140331_bsa.pep.xml),null)\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:282)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:292)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:296)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$7.apply(QueryPlan.scala:301)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:301)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2822)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1121)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:1866)\n\tat sun.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-598ae3ea0d8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#dform.withColumn('@timestamp',lit(unix_timestamp(timestr).cast(\"timestamp\"))).show(2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#dform=dform.withColumn('@ttimestamp', unix_timestamp(concat(lit(timestr),monotonically_increasing_id())).cast(DateType()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@tttimestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munix_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeFmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#dform.withColumn('@tttimestamp',monotonically_increasing_id()).show(2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1491\u001b[0m         \"\"\"\n\u001b[1;32m   1492\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1493\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`20170223172550`' given input columns: [xxxspectrum, xxxindex, xxxretention_time_sec, search_result, xxxend_scan, xxxprecursor_neutral_mass, xxxstart_scan, xxxassumed_charge, xxxspectrumNativeID];;\\n'Project [search_result#982, xxxassumed_charge#983L, xxxend_scan#984L, xxxindex#985L, xxxprecursor_neutral_mass#986, xxxretention_time_sec#987, xxxspectrum#988, xxxspectrumNativeID#989, xxxstart_scan#990L, unix_timestamp('20170223172550, yyyy-MM-dd'T'HH:mm:ss.SSS) AS @tttimestamp#1300]\\n+- Repartition 1, true\\n   +- Relation[search_result#982,xxxassumed_charge#983L,xxxend_scan#984L,xxxindex#985L,xxxprecursor_neutral_mass#986,xxxretention_time_sec#987,xxxspectrum#988,xxxspectrumNativeID#989,xxxstart_scan#990L] XmlRelation(<function0>,Some(/data/promec/new/mzML/140331_bsa.pep.xml),Map(attributeprefix -> xxx, rowtag -> spectrum_query, path -> /data/promec/new/mzML/140331_bsa.pep.xml),null)\\n\""
     ]
    }
   ],
   "source": [
    "#timestr = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "#timestr=unix_timestamp(timestr)\n",
    "#dform.withColumn('@timestamp',lit(unix_timestamp(timestr).cast(\"timestamp\"))).show(2)\n",
    "#dform=dform.withColumn('@ttimestamp', unix_timestamp(concat(lit(timestr),monotonically_increasing_id())).cast(DateType()))\n",
    "df.withColumn('@tttimestamp', lit(unix_timestamp(timestr,timeFmt))).show(2)\n",
    "#dform.withColumn('@tttimestamp',monotonically_increasing_id()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- search_result: struct (nullable = true)\n",
      " |    |-- search_hit: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- search_score: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |-- xxxname: string (nullable = true)\n",
      " |    |    |    |    |    |-- xxxvalue: double (nullable = true)\n",
      " |    |    |    |-- xxxcalc_neutral_pep_mass: double (nullable = true)\n",
      " |    |    |    |-- xxxhit_rank: long (nullable = true)\n",
      " |    |    |    |-- xxxmassdiff: double (nullable = true)\n",
      " |    |    |    |-- xxxnum_matched_ions: long (nullable = true)\n",
      " |    |    |    |-- xxxnum_matched_peptides: long (nullable = true)\n",
      " |    |    |    |-- xxxnum_missed_cleavages: long (nullable = true)\n",
      " |    |    |    |-- xxxnum_tol_term: long (nullable = true)\n",
      " |    |    |    |-- xxxnum_tot_proteins: long (nullable = true)\n",
      " |    |    |    |-- xxxpeptide: string (nullable = true)\n",
      " |    |    |    |-- xxxpeptide_next_aa: string (nullable = true)\n",
      " |    |    |    |-- xxxpeptide_prev_aa: string (nullable = true)\n",
      " |    |    |    |-- xxxprotein: string (nullable = true)\n",
      " |    |    |    |-- xxxtot_num_ions: long (nullable = true)\n",
      " |-- xxxassumed_charge: long (nullable = true)\n",
      " |-- xxxend_scan: long (nullable = true)\n",
      " |-- xxxindex: long (nullable = true)\n",
      " |-- xxxprecursor_neutral_mass: double (nullable = true)\n",
      " |-- xxxretention_time_sec: double (nullable = true)\n",
      " |-- xxxspectrum: string (nullable = true)\n",
      " |-- xxxspectrumNativeID: string (nullable = true)\n",
      " |-- xxxstart_scan: long (nullable = true)\n",
      " |-- @timestamp: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dform.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/promec/new/mzML/140331_bsa.pep.xml20170223204920\n"
     ]
    }
   ],
   "source": [
    "outf=inpf+timestr\n",
    "print(outf)\n",
    "dform.toJSON().saveAsTextFile(outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(attribute_assumed_charge,LongType,true),StructField(attribute_end_scan,LongType,true),StructField(attribute_index,LongType,true),StructField(attribute_precursor_neutral_mass,DoubleType,true),StructField(attribute_retention_time_sec,DoubleType,true),StructField(attribute_spectrum,StringType,true),StructField(attribute_spectrumNativeID,StringType,true),StructField(attribute_start_scan,LongType,true),StructField(search_result,StructType(List(StructField(search_hit,ArrayType(StructType(List(StructField(attribute_calc_neutral_pep_mass,DoubleType,true),StructField(attribute_hit_rank,LongType,true),StructField(attribute_massdiff,DoubleType,true),StructField(attribute_num_matched_ions,LongType,true),StructField(attribute_num_matched_peptides,LongType,true),StructField(attribute_num_missed_cleavages,LongType,true),StructField(attribute_num_tol_term,LongType,true),StructField(attribute_num_tot_proteins,LongType,true),StructField(attribute_peptide,StringType,true),StructField(attribute_peptide_next_aa,StringType,true),StructField(attribute_peptide_prev_aa,StringType,true),StructField(attribute_protein,StringType,true),StructField(attribute_tot_num_ions,LongType,true),StructField(search_score,ArrayType(StructType(List(StructField(_VALUE,StringType,true),StructField(attribute_name,StringType,true),StructField(attribute_value,DoubleType,true))),true),true))),true),true))),true),StructField(_index,StringType,false),StructField(_type,StringType,false),StructField(_id,StringType,true),StructField(@timestamp,StringType,true)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dform.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------+--------+-------------------------+---------------------+--------------------+--------------------+-------------+---------------+\n",
      "|       search_result|xxxassumed_charge|xxxend_scan|xxxindex|xxxprecursor_neutral_mass|xxxretention_time_sec|         xxxspectrum| xxxspectrumNativeID|xxxstart_scan|     @timestamp|\n",
      "+--------------------+-----------------+-----------+--------+-------------------------+---------------------+--------------------+--------------------+-------------+---------------+\n",
      "|[WrappedArray([Wr...|                2|          2|       1|              1241.005467|                  0.9|140331_bsa.00002....|controllerType=0 ...|            2|201702231651170|\n",
      "|[WrappedArray([Wr...|                3|          4|       2|              1495.833029|                  1.6|140331_bsa.00004....|controllerType=0 ...|            4|201702231651171|\n",
      "|[WrappedArray([Wr...|                3|          7|       3|              1355.595755|                  3.0|140331_bsa.00007....|controllerType=0 ...|            7|201702231651172|\n",
      "|[WrappedArray([Wr...|                3|         10|       4|              1496.833243|                  4.3|140331_bsa.00010....|controllerType=0 ...|           10|201702231651173|\n",
      "|[WrappedArray([Wr...|                3|         14|       5|              1430.454123|                  6.3|140331_bsa.00014....|controllerType=0 ...|           14|201702231651174|\n",
      "+--------------------+-----------------+-----------+--------+-------------------------+---------------------+--------------------+--------------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.toJSON().first()\n",
    "#df.toJSON().take(2)\n",
    "#import org.apache.spark.sql.functions.to_json\n",
    "#df.select(to_json(struct($\"_assumed_charge\", $\"search_result\", $\"_precursor_neutral_mass\")))\n",
    "#df.select(\"_assumed_charge\").withColumnRenamed(\"_assumed_charge\", \"x1\")\n",
    "#df.replace(\"_index\", \"aaa\")\n",
    "#df.limit\n",
    "#df.withColumn(\"x7\", dfspectra).first()\n",
    "#df.select('*', (df.attribute_assumed_charge + 10).alias('agePlusTen'))\n",
    "#df.select('*', (dfspectra).alias('agePlusTen'))\n",
    "#df.count()\n",
    "#df.join()\n",
    "#df.select('attribute_assumed_charge').map(lambda x:(x,1)).take(5)\n",
    "#from pyspark.sql.functions import concat, col, lit\n",
    "#df.withColumn('_index', concat(lit('promec'))).show(5)\n",
    "#df.withColumn('_type', concat(lit('search_hit'))).show(5)\n",
    "#df.withColumn('_id', concat(df.attribute_spectrum,lit(dfhdr.select(\"_date\").rdd.flatMap(list).first()))).show(5)\n",
    "#df.withColumn('_id', concat(df.attribute_assumed_charge,dfhdr.select(\"_date\").take(1))).show(5)\n",
    "#dfhdr.first().getInt(0)\n",
    "#from pyspark.sql.types import DateType\n",
    "#import re\n",
    "#from pyspark.sql.functions import regexp_replace, col\n",
    "#df.withColumn('@timestamp', regexp_replace(concat(df.attribute_spectrum,lit(timestr)),r'[^\\d.]+', '').cast(DateType())).show(5)\n",
    "#df.withColumn('@timestamp', (concat(df.attribute_spectrum)).cast(DateType())).show(5)\n",
    "#df.withColumn('@timestamp', regexp_replace(concat(df.attribute_spectrum,lit(timestr)),\"[^\\d.]+\", \"\").cast(DateType())).show(5)\n",
    "#df.withColumn('@timestamp', regexp_replace(concat(df.attribute_spectrum,lit(timestr)),\"[^\\d]+\", \"\").cast(DateType())).show(5)\n",
    "#df.withColumn('@timestamp', regexp_replace(concat(df.attribute_spectrum,monotonically_increasing_id()),\"[^\\d]+\", \"\")).show(5)\n",
    "df.withColumn('@timestamp', concat(lit(timestr),monotonically_increasing_id())).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dfhdr = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='msms_pipeline_analysis').load(inpf)\n",
    "dfparam = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='msms_pipeline_analysis', rowTag=\"search_summary\").load(inpf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfenz = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='msms_pipeline_analysis', rowTag=\"sample_enzyme\").load(inpf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfhdr = sqlContext.read.format('com.databricks.spark.xml').options(rowTag='msms_pipeline_analysis', valueTag='date').load(inpf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-10-27T14:13:48'"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dfenz\n",
    "#dfhdr['_date'][1]\n",
    "#dfhdr.toJSON().first()\n",
    "#dfhdr.show()\n",
    "#df.show()\n",
    "#dfparam.toJSON().take(2)\n",
    "#dfenz.toJSON().first()\n",
    "dfhdr.select(\"_date\").rdd.flatMap(list).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[msms_run_summary: struct<_base_name:string,_msManufacturer:string,_msModel:string,_raw_data:string,_raw_data_type:string,sample_enzyme:struct<_name:string,specificity:struct<_cut:string,_no_cut:string,_sense:string,date:string>>,search_summary:struct<_base_name:string,_fragment_mass_type:string,_precursor_mass_type:string,_search_engine:string,_search_engine_version:string,_search_id:bigint,enzymatic_search_constraint:struct<_enzyme:string,_max_num_internal_cleavages:bigint,_min_number_termini:bigint,date:string>,parameter:array<struct<_name:string,_value:string,date:string>>,search_database:struct<_local_path:string,_type:string,date:string>>,spectrum_query:array<struct<_assumed_charge:bigint,_end_scan:bigint,_index:bigint,_precursor_neutral_mass:double,_retention_time_sec:double,_spectrum:string,_spectrumNativeID:string,_start_scan:bigint,search_result:struct<search_hit:array<struct<_calc_neutral_pep_mass:double,_hit_rank:bigint,_massdiff:double,_num_matched_ions:bigint,_num_matched_peptides:bigint,_num_missed_cleavages:bigint,_num_tol_term:bigint,_num_tot_proteins:bigint,_peptide:string,_peptide_next_aa:string,_peptide_prev_aa:string,_protein:string,_tot_num_ions:bigint,search_score:array<struct<_name:string,_value:double,date:string>>>>>>>>]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selectedData = dfhdr.select(\"_date\").show()\n",
    "#selectedData = dfhdr.select(\"msms_run_summary\")\n",
    "#selectedData = dfhdr.select(\"sample_enzyme\")\n",
    "#df.select(\"search_result\").show()\n",
    "#dfhdr.select(\"search_result\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='msms_pipeline_analysis',rowTag='spectrum_query').load('/data/promec/new/mzML/*_bsa.pep.xml')\n",
    "df.select(\"search_result\").collect()\n",
    "#df.write().json(\"/data/promec/new/mzML/bsa.pep.xml.json\");\n",
    "selectedData = df.select(\"search_result\")\n",
    "#selectedData.write.format(\"com.databricks.spark.xml\").option(\"rootTag\", \"msms_pipeline_analysis\").option(\"rowTag\", \"spectrum_query\").save(\"/data/promec/new/mzML/bsa.pep.xml.json.newbooks.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'saveAsTextFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7fb8d727ddb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mselectedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mselectedData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselectedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mselectedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/promec/new/mzML/bsa.pep.xml.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'saveAsTextFile'"
     ]
    }
   ],
   "source": [
    "selectedData = df.select(\"search_result\")\n",
    "selectedData.printSchema\n",
    "selectedData=selectedData.toJSON\n",
    "selectedData.saveAsTextFile(\"/data/promec/new/mzML/bsa.pep.xml.json\")\n",
    "df.toJSON().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.toJSON().saveAsTextFile(\"/data/promec/new/mzML/bsa.pep.xml.json.files4\")\n",
    "#df.toJSON().map(tuple).saveAsTextFile(\"/data/promec/new/mzML/bsa.pep.xml.json.files2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"aassumed_charge\":2,\"aend_scan\":2,\"aindex\":1,\"aprecursor_neutral_mass\":1241.005467,\"aretention_time_sec\":0.9,\"aspectrum\":\"140331_bsa.00002.00002.2\",\"aspectrumNativeID\":\"controllerType=0 controllerNumber=1 scan=2\",\"astart_scan\":2,\"search_result\":{\"search_hit\":[{\"acalc_neutral_pep_mass\":1356.569457,\"ahit_rank\":1,\"amassdiff\":-115.56399,\"anum_matched_ions\":2,\"anum_matched_peptides\":44155946,\"anum_missed_cleavages\":0,\"anum_tol_term\":2,\"anum_tot_proteins\":1,\"apeptide\":\"GGADDPFQEHER\",\"apeptide_next_aa\":\"G\",\"apeptide_prev_aa\":\"R\",\"aprotein\":\"sp|Q01XA0_REVERSED|SYA_SOLUE\",\"atot_num_ions\":22,\"search_score\":[{\"aname\":\"xcorr\",\"avalue\":0.563},{\"aname\":\"deltacn\",\"avalue\":0.089},{\"aname\":\"deltacnstar\",\"avalue\":0.0},{\"aname\":\"spscore\",\"avalue\":9.4},{\"aname\":\"sprank\",\"avalue\":14.0},{\"aname\":\"expect\",\"avalue\":1.74}]},{\"acalc_neutral_pep_mass\":1897.794113,\"ahit_rank\":2,\"amassdiff\":-656.788647,\"anum_matched_ions\":2,\"anum_matched_peptides\":44155946,\"anum_missed_cleavages\":0,\"anum_tol_term\":2,\"anum_tot_proteins\":1,\"apeptide\":\"MYWSVSDNAEYGGYTR\",\"apeptide_next_aa\":\"G\",\"apeptide_prev_aa\":\"K\",\"aprotein\":\"sp|Q47SB6|ILVC_THEFY\",\"atot_num_ions\":30,\"search_score\":[{\"aname\":\"xcorr\",\"avalue\":0.513},{\"aname\":\"deltacn\",\"avalue\":0.143},{\"aname\":\"deltacnstar\",\"avalue\":0.0},{\"aname\":\"spscore\",\"avalue\":5.5},{\"aname\":\"sprank\",\"avalue\":53.0},{\"aname\":\"expect\",\"avalue\":7.81}]},{\"acalc_neutral_pep_mass\":1324.568394,\"ahit_rank\":3,\"amassdiff\":-83.562927,\"anum_matched_ions\":2,\"anum_matched_peptides\":44155946,\"anum_missed_cleavages\":0,\"anum_tol_term\":2,\"anum_tot_proteins\":2,\"apeptide\":\"ESYTYDAHSPR\",\"apeptide_next_aa\":\"F\",\"apeptide_prev_aa\":\"K\",\"aprotein\":\"sp|B0SRH8_REVERSED|AROC_LEPBP\",\"atot_num_ions\":20,\"search_score\":[{\"aname\":\"xcorr\",\"avalue\":0.483},{\"aname\":\"deltacn\",\"avalue\":0.186},{\"aname\":\"deltacnstar\",\"avalue\":0.0},{\"aname\":\"spscore\",\"avalue\":10.2},{\"aname\":\"sprank\",\"avalue\":9.0},{\"aname\":\"expect\",\"avalue\":19.5}]},{\"acalc_neutral_pep_mass\":1636.725899,\"ahit_rank\":4,\"amassdiff\":-395.720433,\"anum_matched_ions\":2,\"anum_matched_peptides\":44155946,\"anum_missed_cleavages\":0,\"anum_tol_term\":2,\"anum_tot_proteins\":1,\"apeptide\":\"VPMQQSSSCPSMEVK\",\"apeptide_next_aa\":\"D\",\"apeptide_prev_aa\":\"K\",\"aprotein\":\"sp|Q9SHV2_REVERSED|GLR23_ARATH\",\"atot_num_ions\":28,\"search_score\":[{\"aname\":\"xcorr\",\"avalue\":0.459},{\"aname\":\"deltacn\",\"avalue\":0.195},{\"aname\":\"deltacnstar\",\"avalue\":0.0},{\"aname\":\"spscore\",\"avalue\":8.7},{\"aname\":\"sprank\",\"avalue\":21.0},{\"aname\":\"expect\",\"avalue\":40.6}]},{\"acalc_neutral_pep_mass\":1298.603917,\"ahit_rank\":5,\"amassdiff\":-57.59845,\"anum_matched_ions\":2,\"anum_matched_peptides\":44155946,\"anum_missed_cleavages\":0,\"anum_tol_term\":2,\"anum_tot_proteins\":1,\"apeptide\":\"FESIMWEMVK\",\"apeptide_next_aa\":\"N\",\"apeptide_prev_aa\":\"K\",\"aprotein\":\"sp|Q9ZIV1_REVERSED|DNAK_MEGEL\",\"atot_num_ions\":18,\"search_score\":[{\"aname\":\"xcorr\",\"avalue\":0.454},{\"aname\":\"deltacn\",\"avalue\":0.195},{\"aname\":\"deltacnstar\",\"avalue\":0.0},{\"aname\":\"spscore\",\"avalue\":11.2},{\"aname\":\"sprank\",\"avalue\":6.0},{\"aname\":\"expect\",\"avalue\":47.3}]}]}}'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toJSON().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='msms_pipeline_analysis',rowTag='spectrum_query').load('/data/promec/new/mzML/*_bsa.pep.xml')\n",
    "df.select(\"search_result\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark XML, master=spark://10.3.0.128:7077) created by __init__ at <ipython-input-2-d9733dc1196d>:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3f16475c9c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Usage: kmeans <file> <k>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KMeans\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparseVector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    270\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 272\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark XML, master=spark://10.3.0.128:7077) created by __init__ at <ipython-input-2-d9733dc1196d>:5 "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "\n",
    "def parseVector(line):\n",
    "    return np.array([float(x) for x in line.split(' ')])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    lines = sc.textFile(sys.argv[1])\n",
    "    data = lines.map(parseVector)\n",
    "    k = int(sys.argv[2])\n",
    "    model = KMeans.train(data, k)\n",
    "    print(\"Final centers: str(model.clusterCenters)\")\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0K -rw-r--r--. 1 root root 6.0K Nov 11 20:21 /data/promec/new/mzML/131216_bsa.pep.xml\n",
      "   0 -rw-r--r--. 1 root root    0 Nov 14 01:15 /data/promec/new/mzML/140122_bsa.pep.xml\n",
      " 71M -rw-r--r--. 1 root root  71M Nov 12 23:41 /data/promec/new/mzML/140331_bsa.pep.xml\n",
      " 15M -rw-r--r--. 1 root root  15M Nov 14 01:15 /data/promec/new/mzML/140428_bsa.pep.xml\n",
      " 26M -rw-r--r--. 1 root root  26M Nov 14 01:15 /data/promec/new/mzML/141210_bsa.pep.xml\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.xml').options(rootTag='msms_pipeline_analysis',rowTag='spectrum_query').load('/data/promec/new/mzML/130716_bsa1.pep.xml')\n",
    "df.select(\"search_result\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
